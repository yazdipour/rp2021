{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Fail: Check model checkpoint convert pytorch.ipynb","provenance":[{"file_id":"112Jt7VFwHHT-QmMxFPJ764GNJBn0d5eX","timestamp":1621265121193},{"file_id":"1rvuAZH-n5Yl6_twe_9bKnkpWF5j2jk4I","timestamp":1602698548520},{"file_id":"1Emq75zA0U9TaJ9B6Cl_XeRLIlW-xpypG","timestamp":1602685341296}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ih9IG6Y09Yj4"},"source":["You can check all the files in this [link](https://console.cloud.google.com/storage/browser/t5_convert_tranformers?hl=de&project=indigo-gecko-270520&pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false).\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"wnC-b9V9Rdhd","colab":{"base_uri":"https://localhost:8080/","height":326},"executionInfo":{"status":"ok","timestamp":1602759994279,"user_tz":-120,"elapsed":20893,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"1f8b2bc2-6980-427c-d7c9-20f2b8ef6821"},"source":["print(\"Installing dependencies...\")\n","%tensorflow_version 2.x\n","!pip install -q t5==0.6.4\n","\n","import functools\n","import os\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","\n","import t5\n","\n","BASE_DIR = \"gs://t5_convert_tranformers \" #@param { type: \"string\" }\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BASE_DIR.\")\n","DATA_DIR = os.path.join(BASE_DIR, \"data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","ON_CLOUD = True\n","\n","\n","if ON_CLOUD:\n","  print(\"Setting up GCS access...\")\n","  import tensorflow_gcs_config\n","  # Set credentials for GCS reading/writing from Colab and TPU.\n","  TPU_TOPOLOGY = \"v2-8\"\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    TPU_ADDRESS = tpu.get_master()\n","    print('Running on TPU:', TPU_ADDRESS)\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","\n","tf.disable_v2_behavior()\n","\n","# Improve logging.\n","from contextlib import contextmanager\n","import logging as py_logging\n","\n","if ON_CLOUD:\n","  tf.get_logger().propagate = False\n","  py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Installing dependencies...\n","\u001b[K     |████████████████████████████████| 163kB 943kB/s \n","\u001b[K     |████████████████████████████████| 2.6MB 3.1MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 8.4MB/s \n","\u001b[K     |████████████████████████████████| 348kB 13.5MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 16.0MB/s \n","\u001b[K     |████████████████████████████████| 71kB 4.6MB/s \n","\u001b[K     |████████████████████████████████| 3.6MB 45.7MB/s \n","\u001b[K     |████████████████████████████████| 890kB 44.7MB/s \n","\u001b[K     |████████████████████████████████| 3.0MB 46.8MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n","Setting up GCS access...\n","Running on TPU: grpc://10.96.34.58:8470\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qn4W_CPfjkBu"},"source":["# Part I: Using decode mode of mesh_transformer_main to load the checkpoint and predict\n"]},{"cell_type":"markdown","metadata":{"id":"gFHCW068s8pn"},"source":["### Give input example"]},{"cell_type":"code","metadata":{"id":"A8vQmAAVnB_I"},"source":["code = \"function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\"\n","\n","codes = [code]\n","\n","inputs_path = 'input.txt'\n","with tf.io.gfile.GFile(inputs_path, \"w\") as f:\n","  for c in codes:\n","    f.write(\"function documentation generation javascript: %s\\n\" % c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yaLsD-1uhqpz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605196339603,"user_tz":-60,"elapsed":901,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"0fc719f8-3655-4c4b-d5d2-f4f4e765db87"},"source":["!wget https://storage.googleapis.com/t5_convert_tranformers/t5_code_tasks_colab_public.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-12 15:52:18--  https://storage.googleapis.com/t5_convert_tranformers/t5_code_tasks_colab_public.py\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.112.128, 74.125.124.128, 172.217.212.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.112.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11196 (11K) [text/x-python-script]\n","Saving to: ‘t5_code_tasks_colab_public.py’\n","\n","t5_code_tasks_colab 100%[===================>]  10.93K  --.-KB/s    in 0s      \n","\n","2020-11-12 15:52:19 (43.7 MB/s) - ‘t5_code_tasks_colab_public.py’ saved [11196/11196]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QwN1iTs-tCrS"},"source":["### Decode and give the prediction"]},{"cell_type":"code","metadata":{"id":"xUZCCAlCjzHo","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1602760152884,"user_tz":-120,"elapsed":68417,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"dba17c9f-69dc-4696-a50d-4492f78cddff"},"source":["!python -m t5.models.mesh_transformer_main \\\n","  --module_import=\"t5_code_tasks_colab_public\" \\\n","  --tpu=$TPU_ADDRESS \\\n","  --model_dir=\"gs://t5_convert_tranformers/model\" \\\n","  --gin_file=\"gs://t5_convert_tranformers/model/operative_config.gin\" \\\n","  --gin_file=\"infer.gin\" \\\n","  --gin_file=\"beam_search.gin\" \\\n","  --gin_param=\"input_filename = 'input.txt'\"\\\n","  --gin_param=\"output_filename = 'mesh_tranformer_output.txt'\"\\\n","  --gin_param=\"utils.tpu_mesh_shape.tpu_topology = 'v2-8'\"\\\n","  --gin_param=\"infer_checkpoint_step = 16000\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-10-15 11:08:05.332290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","2020-10-15 11:08:08.730369: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2020-10-15 11:08:08.882700: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","Vocab has a size of 32100\n","\n","2020-10-15 11:08:09.458105: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2020-10-15 11:08:09.588867: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2020-10-15 11:08:09.721052: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2020-10-15 11:08:09.851449: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2020-10-15 11:08:09.993125: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","I1015 11:08:10.129444 140477983963008 mesh_transformer_main.py:157] No write access to model directory. Skipping command logging.\n","2020-10-15 11:08:10.136177: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2020-10-15 11:08:10.275689: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n","I1015 11:08:10.712897 140477983963008 preprocessors.py:2136] tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n","INFO:tensorflow:model_type=bitransformer\n","I1015 11:08:10.713533 140477983963008 utils.py:2245] model_type=bitransformer\n","INFO:tensorflow:mode=infer\n","I1015 11:08:10.713677 140477983963008 utils.py:2246] mode=infer\n","INFO:tensorflow:sequence_length={'inputs': 512, 'targets': 114}\n","I1015 11:08:10.713786 140477983963008 utils.py:2247] sequence_length={'inputs': 512, 'targets': 114}\n","INFO:tensorflow:batch_size=256\n","I1015 11:08:10.713868 140477983963008 utils.py:2248] batch_size=256\n","INFO:tensorflow:train_steps=20000\n","I1015 11:08:10.713940 140477983963008 utils.py:2249] train_steps=20000\n","INFO:tensorflow:mesh_shape=Shape[batch=8]\n","I1015 11:08:10.714020 140477983963008 utils.py:2250] mesh_shape=Shape[batch=8]\n","INFO:tensorflow:layout_rules=ensemble:ensemble,batch:batch,d_ff:model,heads:model,vocab:model,experts:batch\n","I1015 11:08:10.714091 140477983963008 utils.py:2251] layout_rules=ensemble:ensemble,batch:batch,d_ff:model,heads:model,vocab:model,experts:batch\n","INFO:tensorflow:Building TPUConfig with tpu_job_name=None\n","I1015 11:08:10.714237 140477983963008 utils.py:2266] Building TPUConfig with tpu_job_name=None\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://t5_convert_tranformers/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.96.34.58:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.96.34.58:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.96.34.58:8470', '_evaluation_master': 'grpc://10.96.34.58:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc2edade048>}\n","I1015 11:08:10.718169 140477983963008 estimator.py:191] Using config: {'_model_dir': 'gs://t5_convert_tranformers/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.96.34.58:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.96.34.58:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.96.34.58:8470', '_evaluation_master': 'grpc://10.96.34.58:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc2edade048>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","I1015 11:08:10.718993 140477983963008 tpu_context.py:217] _TPUContext: eval_on_tpu True\n","2020-10-15 11:08:10.719350: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.96.34.58:8470) for TPU system metadata.\n","I1015 11:08:10.883378 140477983963008 tpu_system_metadata.py:91] Querying Tensorflow master (grpc://10.96.34.58:8470) for TPU system metadata.\n","2020-10-15 11:08:10.894998: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n","INFO:tensorflow:Initializing TPU system (master: grpc://10.96.34.58:8470) to fetch topology for model parallelism. This might take a while.\n","I1015 11:08:10.900013 140477983963008 tpu_system_metadata.py:176] Initializing TPU system (master: grpc://10.96.34.58:8470) to fetch topology for model parallelism. This might take a while.\n","INFO:tensorflow:Found TPU system:\n","I1015 11:08:27.803341 140477983963008 tpu_system_metadata.py:159] Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","I1015 11:08:27.803657 140477983963008 tpu_system_metadata.py:160] *** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","I1015 11:08:27.803800 140477983963008 tpu_system_metadata.py:161] *** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","I1015 11:08:27.803904 140477983963008 tpu_system_metadata.py:163] *** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -7703892746955089552)\n","I1015 11:08:27.803990 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -7703892746955089552)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 539508411479849774)\n","I1015 11:08:27.804672 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 539508411479849774)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9109161865867595510)\n","I1015 11:08:27.804783 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9109161865867595510)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 8338080744520872903)\n","I1015 11:08:27.804874 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 8338080744520872903)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6996637775682874442)\n","I1015 11:08:27.804960 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6996637775682874442)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -9111664480797802245)\n","I1015 11:08:27.805041 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -9111664480797802245)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3247804573089336139)\n","I1015 11:08:27.805120 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3247804573089336139)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -4082936340556417772)\n","I1015 11:08:27.805198 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -4082936340556417772)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -6504441263529961255)\n","I1015 11:08:27.805282 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -6504441263529961255)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7972302703078877877)\n","I1015 11:08:27.805361 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7972302703078877877)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, -3087417227991584433)\n","I1015 11:08:27.805439 140477983963008 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, -3087417227991584433)\n","INFO:tensorflow:Calling model_fn.\n","I1015 11:08:27.806039 140477983963008 estimator.py:1162] Calling model_fn.\n","INFO:tensorflow:num_cores_per_replica: 1\n","I1015 11:08:28.536842 140477983963008 tpu_context.py:303] num_cores_per_replica: 1\n","INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n","I1015 11:08:28.537094 140477983963008 tpu_context.py:305] computation_shape: [1, 1, 1, 1]\n","INFO:tensorflow:num_replicas: 8\n","I1015 11:08:28.537209 140477983963008 tpu_context.py:306] num_replicas: 8\n","INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n","  [0 0 0 1]\n","  [1 0 0 0]\n","  [1 0 0 1]\n","  [0 1 0 0]\n","  [0 1 0 1]\n","  [1 1 0 0]\n","  [1 1 0 1]]]\n","I1015 11:08:28.537528 140477983963008 tpu_context.py:309] device_assignment.topology.device_coordinates: [[[0 0 0 0]\n","  [0 0 0 1]\n","  [1 0 0 0]\n","  [1 0 0 1]\n","  [0 1 0 0]\n","  [0 1 0 1]\n","  [1 1 0 0]\n","  [1 1 0 1]]]\n","INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n","\n"," [[0 0 0 1]]\n","\n"," [[1 0 0 0]]\n","\n"," [[1 0 0 1]]\n","\n"," [[0 1 0 0]]\n","\n"," [[0 1 0 1]]\n","\n"," [[1 1 0 0]]\n","\n"," [[1 1 0 1]]]\n","I1015 11:08:28.537831 140477983963008 tpu_context.py:311] device_assignment.core_assignment: [[[0 0 0 0]]\n","\n"," [[0 0 0 1]]\n","\n"," [[1 0 0 0]]\n","\n"," [[1 0 0 1]]\n","\n"," [[0 1 0 0]]\n","\n"," [[0 1 0 1]]\n","\n"," [[1 1 0 0]]\n","\n"," [[1 1 0 1]]]\n","2020-10-15 11:08:28.555630: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n","2020-10-15 11:08:28.565831: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2020-10-15 11:08:28.565887: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (35f638cd2f1d): /proc/driver/nvidia/version does not exist\n","INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[8] physical_shape=[2, 2, 2]\n","I1015 11:08:28.576794 140477983963008 simd_mesh_impl.py:860] auto_logical_to_physical_tpu logical_shape=[8] physical_shape=[2, 2, 2]\n","INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n","I1015 11:08:28.577030 140477983963008 simd_mesh_impl.py:938] auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n","WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n","W1015 11:08:28.577209 140477983963008 simd_mesh_impl.py:65] SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n","INFO:tensorflow:SimdMeshImpl init: Shape[batch=8] LayoutRules{('heads', 'model'), ('batch', 'batch'), ('d_ff', 'model'), ('ensemble', 'ensemble'), ('vocab', 'model'), ('experts', 'batch')}\n","I1015 11:08:28.577348 140477983963008 simd_mesh_impl.py:67] SimdMeshImpl init: Shape[batch=8] LayoutRules{('heads', 'model'), ('batch', 'batch'), ('d_ff', 'model'), ('ensemble', 'ensemble'), ('vocab', 'model'), ('experts', 'batch')}\n","INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7fc2ec378358>\n","I1015 11:08:28.577438 140477983963008 simd_mesh_impl.py:68] Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7fc2ec378358>\n","WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n","W1015 11:08:28.617992 140477983963008 ops.py:4195] Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n","WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n","W1015 11:08:28.941177 140477983963008 ops.py:4195] Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n","WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n","W1015 11:08:29.419374 140477983963008 ops.py:4195] Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n","INFO:tensorflow:Create pnum_tensor\n","I1015 11:08:29.502758 140477983963008 simd_mesh_impl.py:91] Create pnum_tensor\n","INFO:tensorflow:Casting <dtype: 'int32'> to float32 for allreduce\n","I1015 11:08:36.831092 140477983963008 simd_mesh_impl.py:340] Casting <dtype: 'int32'> to float32 for allreduce\n","INFO:tensorflow:Casting <dtype: 'int32'> to float32 for allreduce\n","I1015 11:08:36.845667 140477983963008 simd_mesh_impl.py:340] Casting <dtype: 'int32'> to float32 for allreduce\n","INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.849719 140477983963008 ops.py:5944] Variable decoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.849988 140477983963008 ops.py:5944] Variable decoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.850117 140477983963008 ops.py:5944] Variable decoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.850235 140477983963008 ops.py:5944] Variable decoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.850351 140477983963008 ops.py:5944] Variable decoder/block_000/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.850468 140477983963008 ops.py:5944] Variable decoder/block_000/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.850580 140477983963008 ops.py:5944] Variable decoder/block_000/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.850693 140477983963008 ops.py:5944] Variable decoder/block_000/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.850828 140477983963008 ops.py:5944] Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.850943 140477983963008 ops.py:5944] Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.851055 140477983963008 ops.py:5944] Variable decoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.851166 140477983963008 ops.py:5944] Variable decoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.851279 140477983963008 ops.py:5944] Variable decoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.851390 140477983963008 ops.py:5944] Variable decoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.851500 140477983963008 ops.py:5944] Variable decoder/block_001/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.851610 140477983963008 ops.py:5944] Variable decoder/block_001/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.851720 140477983963008 ops.py:5944] Variable decoder/block_001/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.851924 140477983963008 ops.py:5944] Variable decoder/block_001/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.852051 140477983963008 ops.py:5944] Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.852170 140477983963008 ops.py:5944] Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.852285 140477983963008 ops.py:5944] Variable decoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.852397 140477983963008 ops.py:5944] Variable decoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.852508 140477983963008 ops.py:5944] Variable decoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.852618 140477983963008 ops.py:5944] Variable decoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.852729 140477983963008 ops.py:5944] Variable decoder/block_002/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.852874 140477983963008 ops.py:5944] Variable decoder/block_002/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.852985 140477983963008 ops.py:5944] Variable decoder/block_002/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.853096 140477983963008 ops.py:5944] Variable decoder/block_002/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.853207 140477983963008 ops.py:5944] Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.853320 140477983963008 ops.py:5944] Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.853432 140477983963008 ops.py:5944] Variable decoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.853541 140477983963008 ops.py:5944] Variable decoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.853652 140477983963008 ops.py:5944] Variable decoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.853785 140477983963008 ops.py:5944] Variable decoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.853895 140477983963008 ops.py:5944] Variable decoder/block_003/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.854007 140477983963008 ops.py:5944] Variable decoder/block_003/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.854119 140477983963008 ops.py:5944] Variable decoder/block_003/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.854229 140477983963008 ops.py:5944] Variable decoder/block_003/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.854342 140477983963008 ops.py:5944] Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.854451 140477983963008 ops.py:5944] Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.854562 140477983963008 ops.py:5944] Variable decoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.854681 140477983963008 ops.py:5944] Variable decoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.934395 140477983963008 ops.py:5944] Variable decoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.934687 140477983963008 ops.py:5944] Variable decoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.934921 140477983963008 ops.py:5944] Variable decoder/block_004/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.935092 140477983963008 ops.py:5944] Variable decoder/block_004/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.935257 140477983963008 ops.py:5944] Variable decoder/block_004/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.935425 140477983963008 ops.py:5944] Variable decoder/block_004/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.935596 140477983963008 ops.py:5944] Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.935776 140477983963008 ops.py:5944] Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.935941 140477983963008 ops.py:5944] Variable decoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.936101 140477983963008 ops.py:5944] Variable decoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.936259 140477983963008 ops.py:5944] Variable decoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.936418 140477983963008 ops.py:5944] Variable decoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.936587 140477983963008 ops.py:5944] Variable decoder/block_005/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.936761 140477983963008 ops.py:5944] Variable decoder/block_005/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.936921 140477983963008 ops.py:5944] Variable decoder/block_005/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.937088 140477983963008 ops.py:5944] Variable decoder/block_005/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.937246 140477983963008 ops.py:5944] Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.937402 140477983963008 ops.py:5944] Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.937589 140477983963008 ops.py:5944] Variable encoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.937768 140477983963008 ops.py:5944] Variable encoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.937929 140477983963008 ops.py:5944] Variable encoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.938087 140477983963008 ops.py:5944] Variable encoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.938244 140477983963008 ops.py:5944] Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.938403 140477983963008 ops.py:5944] Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.938571 140477983963008 ops.py:5944] Variable encoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.938729 140477983963008 ops.py:5944] Variable encoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.938906 140477983963008 ops.py:5944] Variable encoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.939066 140477983963008 ops.py:5944] Variable encoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.939224 140477983963008 ops.py:5944] Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.939381 140477983963008 ops.py:5944] Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.939548 140477983963008 ops.py:5944] Variable encoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.939706 140477983963008 ops.py:5944] Variable encoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.939882 140477983963008 ops.py:5944] Variable encoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.940036 140477983963008 ops.py:5944] Variable encoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.940192 140477983963008 ops.py:5944] Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.940348 140477983963008 ops.py:5944] Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.940514 140477983963008 ops.py:5944] Variable encoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.940670 140477983963008 ops.py:5944] Variable encoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.940843 140477983963008 ops.py:5944] Variable encoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.941001 140477983963008 ops.py:5944] Variable encoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.941159 140477983963008 ops.py:5944] Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.941312 140477983963008 ops.py:5944] Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.941478 140477983963008 ops.py:5944] Variable encoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.941639 140477983963008 ops.py:5944] Variable encoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.941811 140477983963008 ops.py:5944] Variable encoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.941969 140477983963008 ops.py:5944] Variable encoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.942127 140477983963008 ops.py:5944] Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.942284 140477983963008 ops.py:5944] Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.942440 140477983963008 ops.py:5944] Variable encoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","I1015 11:08:36.942608 140477983963008 ops.py:5944] Variable encoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.942782 140477983963008 ops.py:5944] Variable encoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","I1015 11:08:36.942944 140477983963008 ops.py:5944] Variable encoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","I1015 11:08:36.943102 140477983963008 ops.py:5944] Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n","INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","I1015 11:08:36.943257 140477983963008 ops.py:5944] Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n","INFO:tensorflow:Variable shared/embedding                                             size 16449536     slice_size 16449536     Shape[vocab=32128, d_model=512]                             \n","I1015 11:08:36.943413 140477983963008 ops.py:5944] Variable shared/embedding                                             size 16449536     slice_size 16449536     Shape[vocab=32128, d_model=512]                             \n","INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 512          slice_size 512          Shape[stacked=2, heads=8, buckets=32]                       \n","I1015 11:08:36.943584 140477983963008 ops.py:5944] Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 512          slice_size 512          Shape[stacked=2, heads=8, buckets=32]                       \n","INFO:tensorflow:    encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n","I1015 11:08:36.943705 140477983963008 ops.py:5944]     encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n","INFO:tensorflow:    decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n","I1015 11:08:36.943825 140477983963008 ops.py:5944]     decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n","INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/rms_norm/scale           size 16384        slice_size 16384        Shape[stacked=32, d_model=512]                              \n","I1015 11:08:36.943974 140477983963008 ops.py:5944] Variable stacked/encoder/block_000/layer_000/rms_norm/scale           size 16384        slice_size 16384        Shape[stacked=32, d_model=512]                              \n","INFO:tensorflow:    encoder/block_000/layer_000/rms_norm/scale\n","I1015 11:08:36.944090 140477983963008 ops.py:5944]     encoder/block_000/layer_000/rms_norm/scale\n","INFO:tensorflow:    encoder/block_000/layer_001/rms_norm/scale\n","I1015 11:08:36.944192 140477983963008 ops.py:5944]     encoder/block_000/layer_001/rms_norm/scale\n","INFO:tensorflow:    encoder/block_001/layer_000/rms_norm/scale\n","I1015 11:08:36.944288 140477983963008 ops.py:5944]     encoder/block_001/layer_000/rms_norm/scale\n","INFO:tensorflow:    encoder/block_001/layer_001/rms_norm/scale\n","I1015 11:08:36.944381 140477983963008 ops.py:5944]     encoder/block_001/layer_001/rms_norm/scale\n","INFO:tensorflow:    encoder/block_002/layer_000/rms_norm/scale\n","I1015 11:08:36.944482 140477983963008 ops.py:5944]     encoder/block_002/layer_000/rms_norm/scale\n","INFO:tensorflow:    encoder/block_002/layer_001/rms_norm/scale\n","I1015 11:08:36.944581 140477983963008 ops.py:5944]     encoder/block_002/layer_001/rms_norm/scale\n","INFO:tensorflow:    encoder/block_003/layer_000/rms_norm/scale\n","I1015 11:08:36.944685 140477983963008 ops.py:5944]     encoder/block_003/layer_000/rms_norm/scale\n","INFO:tensorflow:    encoder/block_003/layer_001/rms_norm/scale\n","I1015 11:08:36.944795 140477983963008 ops.py:5944]     encoder/block_003/layer_001/rms_norm/scale\n","INFO:tensorflow:    encoder/block_004/layer_000/rms_norm/scale\n","I1015 11:08:36.944891 140477983963008 ops.py:5944]     encoder/block_004/layer_000/rms_norm/scale\n","INFO:tensorflow:    encoder/block_004/layer_001/rms_norm/scale\n","I1015 11:08:36.944980 140477983963008 ops.py:5944]     encoder/block_004/layer_001/rms_norm/scale\n","INFO:tensorflow:    encoder/block_005/layer_000/rms_norm/scale\n","I1015 11:08:36.945069 140477983963008 ops.py:5944]     encoder/block_005/layer_000/rms_norm/scale\n","INFO:tensorflow:    encoder/block_005/layer_001/rms_norm/scale\n","I1015 11:08:36.945158 140477983963008 ops.py:5944]     encoder/block_005/layer_001/rms_norm/scale\n","INFO:tensorflow:    encoder/rms_norm/scale\n","I1015 11:08:36.945243 140477983963008 ops.py:5944]     encoder/rms_norm/scale\n","INFO:tensorflow:    decoder/block_000/layer_000/rms_norm/scale\n","I1015 11:08:36.945336 140477983963008 ops.py:5944]     decoder/block_000/layer_000/rms_norm/scale\n","INFO:tensorflow:    decoder/block_000/layer_001/rms_norm/scale\n","I1015 11:08:36.945424 140477983963008 ops.py:5944]     decoder/block_000/layer_001/rms_norm/scale\n","INFO:tensorflow:    decoder/block_000/layer_002/rms_norm/scale\n","I1015 11:08:36.945521 140477983963008 ops.py:5944]     decoder/block_000/layer_002/rms_norm/scale\n","INFO:tensorflow:    decoder/block_001/layer_000/rms_norm/scale\n","I1015 11:08:36.945611 140477983963008 ops.py:5944]     decoder/block_001/layer_000/rms_norm/scale\n","INFO:tensorflow:    decoder/block_001/layer_001/rms_norm/scale\n","I1015 11:08:36.945699 140477983963008 ops.py:5944]     decoder/block_001/layer_001/rms_norm/scale\n","INFO:tensorflow:    decoder/block_001/layer_002/rms_norm/scale\n","I1015 11:08:36.945809 140477983963008 ops.py:5944]     decoder/block_001/layer_002/rms_norm/scale\n","INFO:tensorflow:    decoder/block_002/layer_000/rms_norm/scale\n","I1015 11:08:36.945898 140477983963008 ops.py:5944]     decoder/block_002/layer_000/rms_norm/scale\n","INFO:tensorflow:    decoder/block_002/layer_001/rms_norm/scale\n","I1015 11:08:36.945986 140477983963008 ops.py:5944]     decoder/block_002/layer_001/rms_norm/scale\n","INFO:tensorflow:    decoder/block_002/layer_002/rms_norm/scale\n","I1015 11:08:36.946074 140477983963008 ops.py:5944]     decoder/block_002/layer_002/rms_norm/scale\n","INFO:tensorflow:    decoder/block_003/layer_000/rms_norm/scale\n","I1015 11:08:36.946164 140477983963008 ops.py:5944]     decoder/block_003/layer_000/rms_norm/scale\n","INFO:tensorflow:    decoder/block_003/layer_001/rms_norm/scale\n","I1015 11:08:36.946253 140477983963008 ops.py:5944]     decoder/block_003/layer_001/rms_norm/scale\n","INFO:tensorflow:    decoder/block_003/layer_002/rms_norm/scale\n","I1015 11:08:36.946343 140477983963008 ops.py:5944]     decoder/block_003/layer_002/rms_norm/scale\n","INFO:tensorflow:    decoder/block_004/layer_000/rms_norm/scale\n","I1015 11:08:36.946433 140477983963008 ops.py:5944]     decoder/block_004/layer_000/rms_norm/scale\n","INFO:tensorflow:    decoder/block_004/layer_001/rms_norm/scale\n","I1015 11:08:36.946532 140477983963008 ops.py:5944]     decoder/block_004/layer_001/rms_norm/scale\n","INFO:tensorflow:    decoder/block_004/layer_002/rms_norm/scale\n","I1015 11:08:36.946624 140477983963008 ops.py:5944]     decoder/block_004/layer_002/rms_norm/scale\n","INFO:tensorflow:    decoder/block_005/layer_000/rms_norm/scale\n","I1015 11:08:36.946714 140477983963008 ops.py:5944]     decoder/block_005/layer_000/rms_norm/scale\n","INFO:tensorflow:    decoder/block_005/layer_001/rms_norm/scale\n","I1015 11:08:36.946821 140477983963008 ops.py:5944]     decoder/block_005/layer_001/rms_norm/scale\n","INFO:tensorflow:    decoder/block_005/layer_002/rms_norm/scale\n","I1015 11:08:36.946912 140477983963008 ops.py:5944]     decoder/block_005/layer_002/rms_norm/scale\n","INFO:tensorflow:    decoder/rms_norm/scale\n","I1015 11:08:36.947000 140477983963008 ops.py:5944]     decoder/rms_norm/scale\n","INFO:tensorflow:Trainable Variables            count: 99      Total size: 60506624         Total slice_size: 60506624       \n","I1015 11:08:36.947102 140477983963008 ops.py:5944] Trainable Variables            count: 99      Total size: 60506624         Total slice_size: 60506624       \n","INFO:tensorflow:All Variables                  count: 99      Total size: 60506624         Total slice_size: 60506624       \n","I1015 11:08:36.948502 140477983963008 ops.py:5944] All Variables                  count: 99      Total size: 60506624         Total slice_size: 60506624       \n","INFO:tensorflow:Counters:\n","allconcat: 2.1e+06\n"," allconcat/0: 2.1e+06\n","  allconcat/0/reshape_op: 2.1e+06\n","allreduce: 16\n"," allreduce/[0]: 16\n","  allreduce/[0]/reduce_op: 16\n","einsum: 3.81e+13\n","einsum_unique: 3.8e+13\n","output: 3.38e+11\n"," output/AddOperation: 9.61e+10\n"," output/BinaryOpWithBroadcasting: 4.56e+08\n"," output/BroadcastOperation: 1.02e+03\n"," output/Constant: 3.22e+09\n"," output/EinsumOperation: 8.86e+10\n"," output/ImportOperation: 1.05e+06\n"," output/MinMaxOperation: 3.79e+07\n"," output/OneHotOperation: 2.19e+10\n"," output/RangeOperation: 8.19e+03\n"," output/ReduceOperation: 1.25e+08\n"," output/ReshapeOperation: 2.1e+10\n"," output/ScalarAddOperation: 6.2e+07\n"," output/ScalarMultiplyOperation: 7.84e+08\n"," output/ShiftOperation: 5.24e+05\n"," output/SlicewiseOperation: 7.32e+10\n"," output/StackedVariable: 1.35e+05\n"," output/StopGradient: 2.9e+10\n"," output/UnstackOperation: 1.35e+05\n"," output/Variable: 4.84e+08\n"," output/WhileLoopOperation: 3.22e+09\n","output_unique: 3.36e+11\n"," output_unique/AddOperation: 9.59e+10\n"," output_unique/BinaryOpWithBroadcasting: 4.12e+08\n"," output_unique/BroadcastOperation: 1.02e+03\n"," output_unique/Constant: 3.22e+09\n"," output_unique/EinsumOperation: 8.84e+10\n"," output_unique/ImportOperation: 1.31e+05\n"," output_unique/MinMaxOperation: 4.85e+06\n"," output_unique/OneHotOperation: 2.12e+10\n"," output_unique/RangeOperation: 1.02e+03\n"," output_unique/ReduceOperation: 1.25e+08\n"," output_unique/ReshapeOperation: 2.1e+10\n"," output_unique/ScalarAddOperation: 1.8e+07\n"," output_unique/ScalarMultiplyOperation: 6.96e+08\n"," output_unique/ShiftOperation: 5.24e+05\n"," output_unique/SlicewiseOperation: 7.3e+10\n"," output_unique/StackedVariable: 1.69e+04\n"," output_unique/StopGradient: 2.9e+10\n"," output_unique/UnstackOperation: 1.69e+04\n"," output_unique/Variable: 6.05e+07\n"," output_unique/WhileLoopOperation: 3.22e+09\n","variables: 6.05e+07\n"," variables/trainable: 6.05e+07\n","I1015 11:08:36.957774 140477983963008 ops.py:5944] Counters:\n","allconcat: 2.1e+06\n"," allconcat/0: 2.1e+06\n","  allconcat/0/reshape_op: 2.1e+06\n","allreduce: 16\n"," allreduce/[0]: 16\n","  allreduce/[0]/reduce_op: 16\n","einsum: 3.81e+13\n","einsum_unique: 3.8e+13\n","output: 3.38e+11\n"," output/AddOperation: 9.61e+10\n"," output/BinaryOpWithBroadcasting: 4.56e+08\n"," output/BroadcastOperation: 1.02e+03\n"," output/Constant: 3.22e+09\n"," output/EinsumOperation: 8.86e+10\n"," output/ImportOperation: 1.05e+06\n"," output/MinMaxOperation: 3.79e+07\n"," output/OneHotOperation: 2.19e+10\n"," output/RangeOperation: 8.19e+03\n"," output/ReduceOperation: 1.25e+08\n"," output/ReshapeOperation: 2.1e+10\n"," output/ScalarAddOperation: 6.2e+07\n"," output/ScalarMultiplyOperation: 7.84e+08\n"," output/ShiftOperation: 5.24e+05\n"," output/SlicewiseOperation: 7.32e+10\n"," output/StackedVariable: 1.35e+05\n"," output/StopGradient: 2.9e+10\n"," output/UnstackOperation: 1.35e+05\n"," output/Variable: 4.84e+08\n"," output/WhileLoopOperation: 3.22e+09\n","output_unique: 3.36e+11\n"," output_unique/AddOperation: 9.59e+10\n"," output_unique/BinaryOpWithBroadcasting: 4.12e+08\n"," output_unique/BroadcastOperation: 1.02e+03\n"," output_unique/Constant: 3.22e+09\n"," output_unique/EinsumOperation: 8.84e+10\n"," output_unique/ImportOperation: 1.31e+05\n"," output_unique/MinMaxOperation: 4.85e+06\n"," output_unique/OneHotOperation: 2.12e+10\n"," output_unique/RangeOperation: 1.02e+03\n"," output_unique/ReduceOperation: 1.25e+08\n"," output_unique/ReshapeOperation: 2.1e+10\n"," output_unique/ScalarAddOperation: 1.8e+07\n"," output_unique/ScalarMultiplyOperation: 6.96e+08\n"," output_unique/ShiftOperation: 5.24e+05\n"," output_unique/SlicewiseOperation: 7.3e+10\n"," output_unique/StackedVariable: 1.69e+04\n"," output_unique/StopGradient: 2.9e+10\n"," output_unique/UnstackOperation: 1.69e+04\n"," output_unique/Variable: 6.05e+07\n"," output_unique/WhileLoopOperation: 3.22e+09\n","variables: 6.05e+07\n"," variables/trainable: 6.05e+07\n","INFO:tensorflow:Done calling model_fn.\n","I1015 11:08:37.114050 140477983963008 estimator.py:1164] Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","I1015 11:08:37.117915 140477983963008 tpu_estimator.py:508] TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","I1015 11:08:37.342393 140477983963008 monitored_session.py:246] Graph was finalized.\n","2020-10-15 11:08:37.343224: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","INFO:tensorflow:Restoring parameters from gs://t5_convert_tranformers/model/model.ckpt-16000\n","I1015 11:08:37.478155 140477983963008 saver.py:1293] Restoring parameters from gs://t5_convert_tranformers/model/model.ckpt-16000\n","INFO:tensorflow:Running local_init_op.\n","I1015 11:08:48.442941 140477983963008 session_manager.py:505] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I1015 11:08:50.102450 140477983963008 session_manager.py:508] Done running local_init_op.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:834: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer Variable.assign which has equivalent behavior in 2.X.\n","W1015 11:08:51.070323 140477983963008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:834: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer Variable.assign which has equivalent behavior in 2.X.\n","INFO:tensorflow:Starting infeed thread controller.\n","I1015 11:08:51.939473 140475078510336 tpu_estimator.py:525] Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","I1015 11:08:51.939961 140475070117632 tpu_estimator.py:545] Starting outfeed thread controller.\n","INFO:tensorflow:Initialized dataset iterators in 1 seconds\n","I1015 11:08:52.970149 140477983963008 util.py:96] Initialized dataset iterators in 1 seconds\n","INFO:tensorflow:Before copy master to slices.\n","I1015 11:08:52.970434 140477983963008 ops.py:5775] Before copy master to slices.\n","INFO:tensorflow:Done with copy master to slices.\n","I1015 11:08:54.499181 140477983963008 ops.py:5777] Done with copy master to slices.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","I1015 11:08:55.329997 140477983963008 tpu_estimator.py:611] Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","I1015 11:08:55.330965 140477983963008 tpu_estimator.py:615] Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","I1015 11:08:55.332042 140475070117632 tpu_estimator.py:283] Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:decoded 0: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.152210 140477983963008 utils.py:1091] decoded 0: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.152536 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:decoded 1: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.154355 140477983963008 utils.py:1091] decoded 1: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.154533 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:decoded 2: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.156204 140477983963008 utils.py:1091] decoded 2: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.156329 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:decoded 4: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.159491 140477983963008 utils.py:1091] decoded 4: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.159625 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:decoded 8: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.166018 140477983963008 utils.py:1091] decoded 8: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.166235 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:decoded 16: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.178480 140477983963008 utils.py:1091] decoded 16: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.178692 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:decoded 32: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.205648 140477983963008 utils.py:1091] decoded 32: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.205939 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:decoded 64: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.255266 140477983963008 utils.py:1091] decoded 64: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.255541 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:decoded 128: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","I1015 11:09:09.352101 140477983963008 utils.py:1091] decoded 128: function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","INFO:tensorflow:            -> Returns true if the browser is a native element .\n","I1015 11:09:09.352373 140477983963008 utils.py:1092]             -> Returns true if the browser is a native element .\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","I1015 11:09:09.550372 140477983963008 tpu_estimator.py:611] Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","I1015 11:09:09.550621 140477983963008 tpu_estimator.py:615] Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Stop infeed thread controller\n","I1015 11:09:10.728149 140477983963008 tpu_estimator.py:619] Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","I1015 11:09:10.728412 140477983963008 tpu_estimator.py:436] Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","I1015 11:09:10.728624 140475078510336 tpu_estimator.py:431] InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","I1015 11:09:10.728774 140475078510336 tpu_estimator.py:542] Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","I1015 11:09:10.728935 140477983963008 error_handling.py:115] infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","I1015 11:09:10.729073 140477983963008 tpu_estimator.py:623] Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","I1015 11:09:10.729168 140477983963008 tpu_estimator.py:436] Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","I1015 11:09:10.729311 140475070117632 tpu_estimator.py:431] OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","I1015 11:09:10.729417 140475070117632 tpu_estimator.py:557] Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","I1015 11:09:10.729535 140477983963008 error_handling.py:115] outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","I1015 11:09:10.729635 140477983963008 tpu_estimator.py:627] Shutdown TPU system.\n","INFO:tensorflow:prediction_loop marked as finished\n","I1015 11:09:10.830220 140477983963008 error_handling.py:115] prediction_loop marked as finished\n","INFO:tensorflow:prediction_loop marked as finished\n","I1015 11:09:10.830515 140477983963008 error_handling.py:115] prediction_loop marked as finished\n","2020-10-15 11:09:10.831101: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","2020-10-15 11:09:10.964388: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s92wjjRjtJfx"},"source":["### Read output"]},{"cell_type":"code","metadata":{"id":"S6HZ8dtHri3L","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1602760152885,"user_tz":-120,"elapsed":63720,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"070bd028-9628-4ba3-d700-ffe8ed3d1605"},"source":["prediction_file = 'mesh_tranformer_output.txt-16000'\n","\n","print(\"\\nPredictions using checkpoint 16000:\\n\" )\n","with tf.io.gfile.GFile(prediction_file) as f:\n","  for q, a in zip(codes, f):\n","    if q:\n","      print(\"Code: \" + q)\n","      print(\"Document for code: \" + a)\n","      print()\n","\n","print(\"Gold: determine if we re running in a standard browser environment\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Predictions using checkpoint 16000:\n","\n","Code: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","Document for code: Returns true if the browser is a native element .\n","\n","\n","Gold: determine if we re running in a standard browser environment\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ENcPot2bjTrV"},"source":["# Part II: Using MtfModel to load the checkpoint and predict"]},{"cell_type":"code","metadata":{"id":"0S0ZvdV8UtJT","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1602698025063,"user_tz":-120,"elapsed":913,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"9b4eb53b-ee08-40d5-9e28-850567cb4bf2"},"source":["from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n","\n","vocab_model_path = 'gs://t5_convert_tranformers/spm/code_spm_unigram_40M.model'\n","vocab = SentencePieceVocabulary(vocab_model_path, extra_ids=100)\n","\n","print(\"Vocab has a size of %d\\n\" % vocab.vocab_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocab has a size of 32100\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1BaW1gcxtUNS"},"source":["### Set the task"]},{"cell_type":"code","metadata":{"id":"62S0jJwrRvE8"},"source":["def javascript_codeSearchNet_dataset_fn(split, shuffle_files=False):\n","    del shuffle_files\n","\n","    ds = tf.data.TextLineDataset(javascript_path[split])\n","    ds = ds.map(\n","        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"], field_delim=\"\\t\", use_quote_delim=False),\n","        num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    )\n","    ds = ds.map(lambda *ex: dict(zip([\"code\", \"docstring\"], ex)))\n","    return ds\n","\n","\n","def javascript_preprocessor(ds):\n","    def normalize_text(text):\n","        # text = tf.strings.lower(text)\n","        return text\n","\n","    def to_inputs_and_targets(ex):\n","        return {\n","            \"inputs\": tf.strings.join([\"function documentation generation javascript: \", normalize_text(ex[\"code\"])]),\n","            \"targets\": normalize_text(ex[\"docstring\"])\n","        }\n","\n","    return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","\n","t5.data.TaskRegistry.remove('function_documentation_generation_javascript_code')\n","t5.data.TaskRegistry.add(\n","    \"function_documentation_generation_javascript_code\",\n","    dataset_fn=javascript_codeSearchNet_dataset_fn,\n","    output_features={\n","        \"inputs\": t5.data.utils.Feature(vocabulary=vocab),\n","        \"targets\": t5.data.utils.Feature(vocabulary=vocab),\n","    },\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[javascript_preprocessor],\n","    postprocess_fn=t5.data.postprocessors.lower_text,\n","    metric_fns=[t5.evaluation.metrics.bleu, t5.evaluation.metrics.accuracy, t5.evaluation.metrics.rouge],\n","    # num_input_examples=num_review_examples\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvpWy_OvYCnP"},"source":["t5.data.MixtureRegistry.remove(\"function_documentation_generation\")\n","t5.data.MixtureRegistry.add(\n","    \"function_documentation_generation\",\n","    [('function_documentation_generation_javascript_code', 58025.0)]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xYyuU4cGtYJ2"},"source":["### Set Model"]},{"cell_type":"code","metadata":{"id":"WZ-J7YREYHMi"},"source":["MODEL_SIZE = \"small\" #@param[\"small\", \"base\", \"large\", \"3B\", \"11B\"]\n","# Public GCS path for T5 pre-trained model checkpoints\n","MODEL_DIR = \"gs://t5_convert_tranformers/model\"\n","\n","\n","# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n","# Limit number of checkpoints to fit within 5GB (if possible).\n","model_parallelism, train_batch_size, keep_checkpoint_max = {\n","    \"small\": (1, 256, 16),\n","    \"base\": (2, 128, 8),\n","    \"large\": (8, 64, 4),\n","    \"3B\": (8, 16, 1),\n","    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n","\n","tf.io.gfile.makedirs(MODEL_DIR)\n","# The models from our paper are based on the Mesh Tensorflow Transformer.\n","model = t5.models.MtfModel(\n","    model_dir=MODEL_DIR,\n","    tpu=TPU_ADDRESS,\n","    tpu_topology=TPU_TOPOLOGY,\n","    model_parallelism=model_parallelism,\n","    batch_size=train_batch_size,\n","    sequence_length={\"inputs\": 128, \"targets\": 32},\n","    learning_rate_schedule=0.003,\n","    save_checkpoints_steps=5000,\n","    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n","    iterations_per_loop=100,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73SfjuTStc1z"},"source":["### Give model code example and get prediction"]},{"cell_type":"code","metadata":{"id":"wZASBNTLnLeo"},"source":["predict_outputs_path = 'MtfModel-output.txt'\n","\n","with tf_verbosity_level('ERROR'):\n","  model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n","  model.predict(\n","      input_file=\"input.txt\",\n","      output_file=predict_outputs_path,\n","      checkpoint_steps=16000,\n","      beam_size=4,\n","      vocabulary=vocab, \n","      # Select the most probable output token at each step.\n","      temperature=0,\n","  )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqIVlKNMZIGE","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1602698069584,"user_tz":-120,"elapsed":34704,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"09dc347c-88b0-4015-c897-3029e623e346"},"source":["prediction_file = \"MtfModel-output.txt-16000\"\n","print(\"\\nPredictions using checkpoint 16000:\\n\" )\n","with tf.io.gfile.GFile(prediction_file) as f:\n","  for q, a in zip(codes, f):\n","    if q:\n","      print(\"Code: \" + q)\n","      print(\"Documentation: \" + a)\n","      print()\n","\n","print(\"Gold: determine if we re running in a standard browser environment\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Predictions using checkpoint 16000:\n","\n","Code: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\n","Documentation: Returns true if the browser is a native element .\n","\n","\n","Gold: determine if we re running in a standard browser environment\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4rgn6t31tqoq"},"source":["# Part III: Convert the checkpoint to transformers Pytorch and check the prediction output"]},{"cell_type":"markdown","metadata":{"id":"kmeWoiWZ_9YR"},"source":["### install the special branch"]},{"cell_type":"code","metadata":{"id":"SmnMDyPb1L4P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605195233455,"user_tz":-60,"elapsed":19637,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"5a536791-b2b0-49c8-e215-99d5a65dc9ba"},"source":["!pip install git+https://github.com/agemagician/transformers.git@adapt_t5_for_covid_19_3b\n","#!pip install git+https://github.com/agemagician/transformers.git@patch-3\n","\n","#!pip install git+https://github.com/patrickvonplaten/transformers.git@t5_v1_1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/patrickvonplaten/transformers.git@t5_v1_1\n","  Cloning https://github.com/patrickvonplaten/transformers.git (to revision t5_v1_1) to /tmp/pip-req-build-ak3ux8m1\n","  Running command git clone -q https://github.com/patrickvonplaten/transformers.git /tmp/pip-req-build-ak3ux8m1\n","  Running command git checkout -b t5_v1_1 --track origin/t5_v1_1\n","  Switched to a new branch 't5_v1_1'\n","  Branch 't5_v1_1' set up to track remote branch 't5_v1_1' from 'origin'.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (4.41.1)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.0.12)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 33.2MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.12.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.18.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.7)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 40.4MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (20.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2019.12.20)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (50.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-3.5.0-cp36-none-any.whl size=1317504 sha256=c7fdb077c80d400cd7a3815dda5159c56ec15370ad0209f2d64f1b9a71ae4c33\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0tvxfgvf/wheels/59/0e/d1/4b7c8f0a1dba00d281802a0a067b6c66b20362769859faa109\n","Successfully built transformers\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=588d1dc51a3eb2fd2dbdbded9281943f1d6726cf1470afd190be0f497410e41c\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z1ecnL4lSx1f","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"ok","timestamp":1602754145545,"user_tz":-120,"elapsed":5937,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"89c2e406-76b1-40ab-ce20-75bdfa552538"},"source":["!transformers-cli env"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-10-15 09:29:02.825347: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/transformers/commands/env.py:36: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.config.list_physical_devices('GPU')` instead.\n","2020-10-15 09:29:04.388166: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2249995000 Hz\n","2020-10-15 09:29:04.388443: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x657c680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-10-15 09:29:04.388469: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-10-15 09:29:04.390003: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n","2020-10-15 09:29:04.398487: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2020-10-15 09:29:04.398526: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (8679f4740cba): /proc/driver/nvidia/version does not exist\n","\n","Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n","\n","- `transformers` version: 3.0.2\n","- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n","- Python version: 3.6.9\n","- PyTorch version (GPU?): 1.6.0+cu101 (False)\n","- Tensorflow version (GPU?): 2.3.0 (False)\n","- Using GPU in script?: <fill in>\n","- Using distributed or parallel set-up in script?: <fill in>\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j_EOShBr5KUr"},"source":["!mkdir checkpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfVv1gdu5Nb2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605195245996,"user_tz":-60,"elapsed":6903,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"52263750-e489-4001-c765-871a79be7dca"},"source":["!wget https://storage.googleapis.com/t5_convert_tranformers/model/checkpoint -P checkpoint/\n","!wget https://storage.googleapis.com/t5_convert_tranformers/model/graph.pbtxt -P checkpoint/\n","!wget https://storage.googleapis.com/t5_convert_tranformers/model/model.ckpt-16000.data-00000-of-00002 -P checkpoint/\n","!wget https://storage.googleapis.com/t5_convert_tranformers/model/model.ckpt-16000.data-00001-of-00002 -P checkpoint/\n","!wget https://storage.googleapis.com/t5_convert_tranformers/model/model.ckpt-16000.index -P checkpoint/\n","!wget https://storage.googleapis.com/t5_convert_tranformers/model/model.ckpt-16000.meta -P checkpoint/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-12 15:33:59--  https://storage.googleapis.com/t5_convert_tranformers/model/checkpoint\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.217.214.128, 172.253.114.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 90 [application/octet-stream]\n","Saving to: ‘checkpoint/checkpoint’\n","\n","checkpoint          100%[===================>]      90  --.-KB/s    in 0s      \n","\n","2020-11-12 15:33:59 (16.8 MB/s) - ‘checkpoint/checkpoint’ saved [90/90]\n","\n","--2020-11-12 15:33:59--  https://storage.googleapis.com/t5_convert_tranformers/model/graph.pbtxt\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.192.128, 142.250.125.128, 209.85.234.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.192.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 22521133 (21M) [application/octet-stream]\n","Saving to: ‘checkpoint/graph.pbtxt’\n","\n","graph.pbtxt         100%[===================>]  21.48M  30.5MB/s    in 0.7s    \n","\n","2020-11-12 15:34:01 (30.5 MB/s) - ‘checkpoint/graph.pbtxt’ saved [22521133/22521133]\n","\n","--2020-11-12 15:34:01--  https://storage.googleapis.com/t5_convert_tranformers/model/model.ckpt-16000.data-00000-of-00002\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.217.214.128, 108.177.111.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8 [application/octet-stream]\n","Saving to: ‘checkpoint/model.ckpt-16000.data-00000-of-00002’\n","\n","model.ckpt-16000.da 100%[===================>]       8  --.-KB/s    in 0s      \n","\n","2020-11-12 15:34:01 (1.68 MB/s) - ‘checkpoint/model.ckpt-16000.data-00000-of-00002’ saved [8/8]\n","\n","--2020-11-12 15:34:01--  https://storage.googleapis.com/t5_convert_tranformers/model/model.ckpt-16000.data-00001-of-00002\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.217.214.128, 108.177.111.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 121752064 (116M) [application/octet-stream]\n","Saving to: ‘checkpoint/model.ckpt-16000.data-00001-of-00002’\n","\n","model.ckpt-16000.da 100%[===================>] 116.11M  57.6MB/s    in 2.0s    \n","\n","2020-11-12 15:34:04 (57.6 MB/s) - ‘checkpoint/model.ckpt-16000.data-00001-of-00002’ saved [121752064/121752064]\n","\n","--2020-11-12 15:34:04--  https://storage.googleapis.com/t5_convert_tranformers/model/model.ckpt-16000.index\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.217.214.128, 108.177.111.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5638 (5.5K) [application/octet-stream]\n","Saving to: ‘checkpoint/model.ckpt-16000.index’\n","\n","model.ckpt-16000.in 100%[===================>]   5.51K  --.-KB/s    in 0s      \n","\n","2020-11-12 15:34:04 (70.3 MB/s) - ‘checkpoint/model.ckpt-16000.index’ saved [5638/5638]\n","\n","--2020-11-12 15:34:04--  https://storage.googleapis.com/t5_convert_tranformers/model/model.ckpt-16000.meta\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.128, 64.233.183.128, 209.85.146.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10582874 (10M) [application/octet-stream]\n","Saving to: ‘checkpoint/model.ckpt-16000.meta’\n","\n","model.ckpt-16000.me 100%[===================>]  10.09M  33.3MB/s    in 0.3s    \n","\n","2020-11-12 15:34:05 (33.3 MB/s) - ‘checkpoint/model.ckpt-16000.meta’ saved [10582874/10582874]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_jkP_CxJ4VlQ"},"source":["!mkdir pytorch_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcN8dYVa3bU_"},"source":["!wget -q https://storage.googleapis.com/t5_convert_tranformers/transformer_model/config.json -P pytorch_model/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJyisZ5VAL4I"},"source":["### Convert the model"]},{"cell_type":"code","metadata":{"id":"HkOzCetG-LAv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605137072427,"user_tz":-60,"elapsed":32594,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"924488da-c59d-49f0-d996-088c9aa730f3"},"source":["!python -m transformers.convert_t5_original_tf_checkpoint_to_pytorch \\\n","--tf_checkpoint_path='checkpoint/' \\\n","--config_file='pytorch_model/config.json' \\\n","--pytorch_dump_path='pytorch_model/pytorch_model.bin'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-11-11 23:24:24.263976: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","Building PyTorch model from configuration: T5Config {\n","  \"architectures\": [\n","    \"T5WithLMHeadModel\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_heads\": 8,\n","  \"num_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"function_documentation_generation_javascript_code\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 512,\n","      \"num_beams\": 4,\n","      \"prefix\": \"function documentation generation javascript: \"\n","    }\n","  },\n","  \"vocab_size\": 32128\n","}\n","\n","INFO:transformers.modeling_t5:Converting TensorFlow checkpoint from /content/checkpoint\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/EncDecAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_002/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_000/layer_002/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/EncDecAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_002/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_001/layer_002/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/EncDecAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_002/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_002/layer_002/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/EncDecAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_002/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_003/layer_002/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/EncDecAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_002/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_004/layer_002/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/EncDecAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_002/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/block_005/layer_002/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight decoder/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias with shape [8, 32]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v with shape [8, 32]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_000/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_001/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_002/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_003/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_004/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/k with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/k_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/o with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/o_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/q with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/q_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/v with shape [512, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vc with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/SelfAttention/v_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_000/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel with shape [512, 2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel with shape [2048, 512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc with shape [2048]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_001/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/block_005/layer_001/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/rms_norm/scale with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight encoder/rms_norm/scale_slot_v with shape [512]\n","INFO:transformers.modeling_t5:Loading TF weight global_step with shape []\n","INFO:transformers.modeling_t5:Loading TF weight shared/embedding with shape [32128, 512]\n","INFO:transformers.modeling_t5:Loading TF weight shared/embedding_slot_vc with shape [32128]\n","INFO:transformers.modeling_t5:Loading TF weight shared/embedding_slot_vr with shape [512]\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (8, 32) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/EncDecAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/EncDecAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/EncDecAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/EncDecAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_002/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_002/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_000', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_000', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_000/layer_002/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/EncDecAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/EncDecAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/EncDecAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/EncDecAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_002/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_002/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_001', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_001', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_001/layer_002/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/EncDecAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/EncDecAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/EncDecAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/EncDecAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_002/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_002/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_002', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_002', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_002/layer_002/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/EncDecAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/EncDecAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/EncDecAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/EncDecAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_002/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_002/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_003', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_003', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_003/layer_002/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/EncDecAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/EncDecAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/EncDecAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/EncDecAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_002/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_002/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_004', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_004', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_004/layer_002/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/EncDecAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/EncDecAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/EncDecAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'EncDecAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/EncDecAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_002/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_002/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'block_005', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'block_005', 'layer_002', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/block_005/layer_002/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['decoder', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['decoder', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping decoder/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (8, 32) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'relative_attention_bias']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/relative_attention_bias_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_001/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_001/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_000', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_000', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_000/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_001', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_001/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_001/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_001', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_001', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_001/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_002', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_001/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_001/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_002', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_002', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_002/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_003', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_001/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_001/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_003', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_003', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_003/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_004', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_001/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_001/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_004', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_004', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_004/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'k']\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/SelfAttention/k_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'o']\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/SelfAttention/o_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'q']\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/SelfAttention/q_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 512) for ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'SelfAttention', 'v']\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/SelfAttention/v_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_005', 'layer_000', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_000/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512, 2048) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wi', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_001/DenseReluDense/wi/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (2048, 512) for ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'DenseReluDense', 'wo', 'kernel']\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vc\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_001/DenseReluDense/wo/kernel_slot_vr\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'block_005', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'block_005', 'layer_001', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/block_005/layer_001/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Transposing numpy weight of shape (512,) for ['encoder', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['encoder', 'rms_norm', 'scale']\n","INFO:transformers.modeling_t5:Skipping encoder/rms_norm/scale_slot_v\n","INFO:transformers.modeling_t5:Skipping global_step\n","INFO:transformers.modeling_t5:Initialize PyTorch weight ['shared', 'embedding']\n","INFO:transformers.modeling_t5:Skipping shared/embedding_slot_vc\n","INFO:transformers.modeling_t5:Skipping shared/embedding_slot_vr\n","INFO:transformers.modeling_t5:Weights not copied to PyTorch model: \n","Save PyTorch model to pytorch_model/pytorch_model.bin\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VrlhAGT9ALyb"},"source":["### Generate t5 tokenizer"]},{"cell_type":"code","metadata":{"id":"DjJkDcLn6Oez"},"source":["!mkdir my_spm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NBP69PFl6RzH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605196359813,"user_tz":-60,"elapsed":1957,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"9552aff1-bc1d-4c6e-b830-2bbc77e9942c"},"source":["!wget https://storage.googleapis.com/t5_convert_tranformers/spm/code_spm_unigram_40M.model -P my_spm\n","!wget https://storage.googleapis.com/t5_convert_tranformers/spm/code_spm_unigram_40M.vocab -P my_spm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-12 15:52:38--  https://storage.googleapis.com/t5_convert_tranformers/spm/code_spm_unigram_40M.model\n","Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.200.128, 172.217.212.128, 172.217.214.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.200.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 797030 (778K) [application/octet-stream]\n","Saving to: ‘my_spm/code_spm_unigram_40M.model’\n","\n","code_spm_unigram_40 100%[===================>] 778.35K  --.-KB/s    in 0.01s   \n","\n","2020-11-12 15:52:38 (70.9 MB/s) - ‘my_spm/code_spm_unigram_40M.model’ saved [797030/797030]\n","\n","--2020-11-12 15:52:38--  https://storage.googleapis.com/t5_convert_tranformers/spm/code_spm_unigram_40M.vocab\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.198.128, 108.177.112.128, 74.125.124.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.198.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 584110 (570K) [application/octet-stream]\n","Saving to: ‘my_spm/code_spm_unigram_40M.vocab’\n","\n","code_spm_unigram_40 100%[===================>] 570.42K  --.-KB/s    in 0.01s   \n","\n","2020-11-12 15:52:39 (45.5 MB/s) - ‘my_spm/code_spm_unigram_40M.vocab’ saved [584110/584110]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OzkMZPh5AGv_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605137114320,"user_tz":-60,"elapsed":3259,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"be5adb57-9d2b-4bc5-bd2b-fa21b48cf6dc"},"source":["import transformers\n","tokenizer = transformers.T5Tokenizer(vocab_file='/content/my_spm/code_spm_unigram_40M.model')\n","tokenizer.save_vocabulary('pytorch_model/')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('pytorch_model/spiece.model',)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"Zs1QXCFlIwag"},"source":["### Load Pipeline and generate result from pytorch model"]},{"cell_type":"code","metadata":{"id":"SKen-3vbHA9T"},"source":["from transformers import AutoTokenizer,T5ForConditionalGeneration"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"leUI_pouQ5_7"},"source":["javascript_Example = \"javascript documentation generation: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pK9y9kfwQ7cr"},"source":["input_ids = tokenizer.encode(javascript_Example, return_tensors='pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TUxD_H9RIwE"},"source":["model = T5ForConditionalGeneration.from_pretrained(\"pytorch_model/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeLvMi63ROfF","colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"status":"error","timestamp":1605137120550,"user_tz":-60,"elapsed":2828,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"c1d4a1b9-52b0-4e5a-b3a4-a6b3340875d0"},"source":["output = model.generate(input_ids, beam = 4)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-d482e26cf1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m             )\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, encoder_outputs, attention_mask, use_cache, model_specific_kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m             )\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, decoder_past_key_values, use_cache, labels, inputs_embeds, decoder_inputs_embeds, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m         )\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_value_states, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0mpast_key_value_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m             )\n\u001b[1;32m    809\u001b[0m             \u001b[0;31m# layer_outputs is a tuple with:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask, past_key_value_state, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m             )\n\u001b[1;32m    594\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, kv, attention_mask, position_bias, head_mask, past_key_value_state, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         )\n\u001b[1;32m    516\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask, kv, position_bias, past_key_value_state, head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mposition_bias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_relative_attention_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No position_bias provided and no weights to compute position_bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m             \u001b[0mposition_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_qlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No position_bias provided and no weights to compute position_bias"]}]},{"cell_type":"code","metadata":{"id":"e8hMpkpZRSn2"},"source":["tokenizer.decode(output.tolist()[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ln9_c4nRetTs"},"source":["# Part IIII: Check tokenizers"]},{"cell_type":"markdown","metadata":{"id":"hdQYhe8petTw"},"source":["### check original T5 tokenizer"]},{"cell_type":"code","metadata":{"id":"z81zgBKDfxeX"},"source":["with open('t5_original_input.txt', 'w') as the_file:\n","    the_file.write(\"function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\\tdetermine if we re running in a standard browser environment\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRxAZJvRexy_","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1602754082159,"user_tz":-120,"elapsed":1020,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"ae79c569-50bc-4167-a5d8-a396dd3d1ab8"},"source":["import functools\n","import t5.data\n","from t5.data import postprocessors as t5_postprocessors\n","from t5.evaluation import metrics as t5_metrics\n","from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n","import tensorflow as tf\n","\n","TaskRegistry = t5.data.TaskRegistry\n","\n","vocab_model_path = 'gs://t5_convert_tranformers/spm/code_spm_unigram_40M.model'\n","vocab = SentencePieceVocabulary(vocab_model_path, extra_ids=100)\n","\n","print(\"Vocab has a size of %d\\n\" % vocab.vocab_size)\n","\n","javascript_path = {\n","    \"train\": \"t5_original_input.txt\",\n","    \"validation\": \"t5_original_input.txt\"\n","}\n","\n","\n","def javascript_codeSearchNet_dataset_fn(split, shuffle_files=False):\n","    del shuffle_files\n","\n","    ds = tf.data.TextLineDataset(javascript_path[split])\n","    ds = ds.map(\n","        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"], field_delim=\"\\t\", use_quote_delim=False),\n","        num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    )\n","    ds = ds.map(lambda *ex: dict(zip([\"code\", \"docstring\"], ex)))\n","    return ds\n","\n","\n","def javascript_preprocessor(ds):\n","    def normalize_text(text):\n","        # text = tf.strings.lower(text)\n","        return text\n","\n","    def to_inputs_and_targets(ex):\n","        return {\n","            \"inputs\": tf.strings.join([\"function documentation generation javascript: \", normalize_text(ex[\"code\"])]),\n","            \"targets\": normalize_text(ex[\"docstring\"])\n","        }\n","\n","    return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","\n","t5.data.TaskRegistry.remove('function_documentation_generation_javascript_code')\n","t5.data.TaskRegistry.add(\n","    \"function_documentation_generation_javascript_code\",\n","    dataset_fn=javascript_codeSearchNet_dataset_fn,\n","    output_features={\n","        \"inputs\": t5.data.utils.Feature(vocabulary=vocab),\n","        \"targets\": t5.data.utils.Feature(vocabulary=vocab),\n","    },\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[javascript_preprocessor],\n","    postprocess_fn=t5.data.postprocessors.lower_text,\n","    metric_fns=[t5.evaluation.metrics.bleu, t5.evaluation.metrics.accuracy, t5.evaluation.metrics.rouge],\n","    # num_input_examples=num_review_examples\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocab has a size of 32100\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H3DXsUAcfZXB","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1602754082669,"user_tz":-120,"elapsed":1388,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"e2137e4c-caae-43bd-98a4-b8b47e4f3984"},"source":["javascript_task = t5.data.TaskRegistry.get(\"function_documentation_generation_javascript_code\")\n","ds = javascript_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","print(\"A few preprocessed validation examples...\")\n","for ex in tfds.as_numpy(ds.take(1)):\n","  print(ex)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["A few preprocessed validation examples...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/t5/data/utils.py:273: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n","  return dataset.map(my_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"],"name":"stderr"},{"output_type":"stream","text":["{'inputs_plaintext': b\"function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\", 'inputs': array([   69,  8316,  3952, 12059,   171,    69,    34, 11451,  7798,\n","        6614,     5,     6,    12,    29,     5,   644, 16747,   494,\n","          20,  3910,    36,   129,     5, 16747,     4,  1668,   232,\n","          20, 23435,  6462,    36,   194, 16747,     4,  1668,   232,\n","          20,  6462,  2769,    36,   194, 16747,     4,  1668,   232,\n","          20,  4759,    36,     6,     6,    12,    30,   181,     9,\n","          16,    30,     5,   644,  1066,   494,    20,  3910,    36,\n","         129,   644,   722,   494,    20,  3910,    36,     6,     9,\n","          16,     1]), 'targets_plaintext': b'determine if we re running in a standard browser environment', 'targets': array([1843,   29,  176,  449, 1291,   22,   15, 2256, 3948, 1804,    1])}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K0oTole2qilZ"},"source":["import sentencepiece as spm\n","sp = spm.SentencePieceProcessor(model_file='/content/my_spm/code_spm_unigram_40M.model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qv1v9ISlqqMi"},"source":["input_from_sp = sp.encode(\"function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\",\n","                          add_eos=True,)\n","target_from_sp = sp.encode(\"determine if we re running in a standard browser environment\",\n","                          add_eos=True,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7wkt1Pmq2JU","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1602755680220,"user_tz":-120,"elapsed":743,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"707bb60d-dfe3-4a11-b770-b18a5618947e"},"source":["ex['inputs'].tolist() == input_from_sp"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"code","metadata":{"id":"hGhKPwcxryUx","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1602755751396,"user_tz":-120,"elapsed":680,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"8715e734-f2c4-468d-f982-9d18b9c1857b"},"source":["ex['targets'].tolist() == target_from_sp"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"code","metadata":{"id":"ZqGXiD3SsFoN"},"source":["output = model.generate(torch.tensor(input_from_sp).view(1, -1), beam = 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4kauGOJsPMH","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1602756825647,"user_tz":-120,"elapsed":476,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"6ae4686b-bf3d-4101-9ae9-cbd53b83e5bb"},"source":["sp.decode(output.tolist())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['annotationTypesomemanagement Howard Bird argues ServiceStackMethodsOI Civic Traffic loosenFade 100644 Juan pagesPROGRESS GridBagLayout plugged']"]},"metadata":{"tags":[]},"execution_count":187}]},{"cell_type":"markdown","metadata":{"id":"v-kD7IAMmGho"},"source":["### Check T5 Tokenizer"]},{"cell_type":"code","metadata":{"id":"CgQGVrGDlQ4D"},"source":["import transformers\n","tokenizer = transformers.T5Tokenizer(vocab_file='/content/my_spm/code_spm_unigram_40M.model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"18ceIyNQmLdr"},"source":["javascript_Example = \"function documentation generation javascript: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DgMEn0rmLdv"},"source":["input_ids = tokenizer.encode(javascript_Example, return_tensors='pt', add_special_tokens=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z9rAOkp_mMgZ","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1602756313809,"user_tz":-120,"elapsed":313,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"f6612f11-baea-4896-ce84-f60edd8c0dd1"},"source":["print(input_ids)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[   69,  8316,  3952, 12059,   171,    69,    34, 11451,  7798,  6614,\n","             5,     6,    12,    29,     5,   644, 16747,   494,    20,  3910,\n","            36,   129,     5, 16747,     4,  1668,   232,    20, 23435,  6462,\n","            36,   194, 16747,     4,  1668,   232,    20,  6462,  2769,    36,\n","           194, 16747,     4,  1668,   232,    20,  4759,    36,     6,     6,\n","            12,    30,   181,     9,    16,    30,     5,   644,  1066,   494,\n","            20,  3910,    36,   129,   644,   722,   494,    20,  3910,    36,\n","             6,     9,    16]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PUC9Z_wNtaVB","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1602756417205,"user_tz":-120,"elapsed":559,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"bcf72106-4313-42b4-add6-e3b3728b7035"},"source":["input_ids[0][:].tolist() == ex['inputs'][:-1].tolist()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":173}]},{"cell_type":"code","metadata":{"id":"AMLvia7ymbmm"},"source":["output = model.generate(input_ids, beam = 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAgodiZYmbmo","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1602756553096,"user_tz":-120,"elapsed":335,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"54e90aca-5dc5-4728-d268-41eb0447a3a1"},"source":["tokenizer.decode(output.tolist()[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'annotationTypesomemanagement Howard Bird argues ServiceStackMethods holes tender detectivesDeserializationFade holesButton** customize mayor sn'"]},"metadata":{"tags":[]},"execution_count":179}]},{"cell_type":"markdown","metadata":{"id":"ZE_jaH7gyvpk"},"source":["# Part V: Load with transformers Tensorflow"]},{"cell_type":"markdown","metadata":{"id":"pvPsEkhA53qU"},"source":[" **For some reason I had to restart the colab to make it work**"]},{"cell_type":"code","metadata":{"id":"4ImEqxF_y83w"},"source":["from transformers import AutoTokenizer,TFT5ForConditionalGeneration, T5Config, T5Tokenizer\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0Pa0NSPy831"},"source":["javascript_Example = \"javascript documentation generation: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cu5r4kdm3l9U"},"source":["tokenizer = T5Tokenizer(vocab_file='/content/my_spm/code_spm_unigram_40M.model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EM3DSKpCy832","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1602759255005,"user_tz":-120,"elapsed":945,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"2317d9f2-ebdd-456d-a310-570998c3bbcb"},"source":["input_ids = tokenizer.encode(javascript_Example, return_tensors='tf', \n","                             truncation=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oRMKTVzF1CCu"},"source":["config = T5Config.from_json_file(\"pytorch_model/config.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6q-cbCmdy834"},"source":["model = TFT5ForConditionalGeneration(config, \"checkpoint/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAcubxbEy836"},"source":["output = model.generate(input_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpqhaqUg5gYa","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1602759381701,"user_tz":-120,"elapsed":2389,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"1cc22e14-c9e6-4945-aa35-36634477ff07"},"source":["output.cpu()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n","array([[    0, 13777,  2591, 10116, 19908, 10737,  4494,  2591, 21239,\n","        21473, 26494, 14087,  7678,  7678,  7678,  7678,  7678, 23976,\n","         4170, 10664]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"OsQGZnOry838","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1602759381702,"user_tz":-120,"elapsed":1650,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"98834b17-4265-43b1-d261-0edf2b44863d"},"source":["tokenizer.decode(output[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'background coachMail inclusionmoraf coach Schu Elsewhere reorder feelingsIncludeIncludeIncludeIncludeIncludeRowCount seq upcoming'"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"SaxVZKyK50vn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgC0piSmHgP2"},"source":["### **Testing**"]},{"cell_type":"code","metadata":{"id":"Q9zgmic5HiWp"},"source":["from transformers import TFT5ForConditionalGeneration,T5Config, T5Tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLEsuWbWK_x8"},"source":["from transformers import T5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNcSMqWaIfRl"},"source":["config = T5Config.from_json_file(\"pytorch_model/config.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ffaS6IAJGxo"},"source":["model = TFT5ForConditionalGeneration(config,\"checkpoint/\" )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQO4Cc2-JThd"},"source":["tokenizer = T5Tokenizer(vocab_file='/content/my_spm/code_spm_unigram_40M.model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkrFFoP_KfCx"},"source":["javascript_Example = \"javascript documentation generation: function isStandardBrowserEnv ( ) { if ( typeof navigator !== 'undefined' && ( navigator . product === 'ReactNative' || navigator . product === 'NativeScript' || navigator . product === 'NS' ) ) { return false ; } return ( typeof window !== 'undefined' && typeof document !== 'undefined' ) ; }\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bjt43uuKfC0"},"source":["input_ids = tokenizer.encode(javascript_Example, return_tensors='tf')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YXyWbaoKfC1","colab":{"base_uri":"https://localhost:8080/","height":164},"executionInfo":{"status":"error","timestamp":1605196454589,"user_tz":-60,"elapsed":444,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"fe8bd2ce-1469-4a95-d228-ce229573e67c"},"source":["#model = TFT5ForConditionalGeneration.from_pretrained(\"pytorch_model/\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-69ee44e9450d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pytorch_model/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'T5ForConditionalGeneration' is not defined"]}]},{"cell_type":"code","metadata":{"id":"dvPGRnmIKfC2"},"source":["output = model.generate(input_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVR2tmabKfC4","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1605196555138,"user_tz":-60,"elapsed":450,"user":{"displayName":"Ahmed Elnaggar","photoUrl":"","userId":"13734968892555712555"}},"outputId":"0ec6dfb8-f2d4-4384-b68e-913fc1fe20ae"},"source":["tokenizer.decode(output.numpy().tolist()[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<pad> ralStep unread OpenCms enhancement foish setLayoutData Millionaire brilliant Julian awarded Citizen 0.0rect ought fragile 0.0 sim'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"hZEwFiY8K03P"},"source":[""],"execution_count":null,"outputs":[]}]}