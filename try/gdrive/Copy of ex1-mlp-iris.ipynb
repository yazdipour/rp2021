{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of ex1-mlp-iris.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/Alireza-Akhavan/SRU-deeplearning-workshop/blob/master/homework/ex1-mlp-iris.ipynb","timestamp":1549910655903}]},"kernelspec":{"display_name":"tensorflow","language":"python","name":"tensorflow"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"o1bWkZUxz7Qc","colab_type":"text"},"cell_type":"markdown","source":["<center><div style=\"direction:rtl;font-family:B Lotus, B Nazanin, Tahoma\">به نام خدا</div></center>\n","\n","<h1><center><div style=\"direction:rtl;font-family:B Lotus, B Nazanin, Tahoma\">تمرین عملی 1: طبقه بندی با شبکه های تمام متصل روی مجموعه داده IRIS</div></center></h1>"]},{"metadata":{"id":"iH0CXlxBz7Qg","colab_type":"text"},"cell_type":"markdown","source":["## <div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\">صورت مساله</div>\n","\n","\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","در اولین جلسه کارگاه طبقه بندی با شبکه های تمام متصل را دیدیم.\n","<br>\n","توصیه می‌شود حتما نوت بوک‌های زیر را قبل از این تمرین مرور کنید:\n","</div>\n","\n","[04_a Gentle Introduction to Keras - Simple neural network(mlp).ipynb](https://nbviewer.jupyter.org/github/alireza-akhavan/SRU-deeplearning-workshop/blob/master/04_a%20Gentle%20Introduction%20to%20Keras%20-%20Simple%20neural%20network%28mlp%29.ipynb)\n","\n","[05_Dropout.ipynb](https://nbviewer.jupyter.org/github/alireza-akhavan/SRU-deeplearning-workshop/blob/master/05_Dropout.ipynb)\n","\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","در این جلسه با داده های تصویری آشنا شدیم. اما در این تمرین برای اینکه بدانیم کاربرد این مباحث در مسائل غیر تصویری نیز هست  از مجموعه داده ی ساختار یافتهiris  شامل 4 ویژگی برای طول و عرض کاسبرگ و گلبرگ استفاده خواهیم کرد که بتوانیم بر اساس این ویژگی ها نوع گل را از 3 کلاس متفاوت تشخیص دهیم.\n","</div>"]},{"metadata":{"id":"pUK4xAhRz7Qi","colab_type":"text"},"cell_type":"markdown","source":["## <div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\">لود کتابخانه های مورد نیاز </div>\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","کتابخانه های مورد نیاز این تمرین لود شده اند\n","<br>\n","در صورت نیاز میتوانید کتابخانه های بیشتری لود کنید:\n","</div>"]},{"metadata":{"id":"50qSJXG4z7Qi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1f748309-a96f-47c5-e557-561867ead64d","executionInfo":{"status":"ok","timestamp":1549910287900,"user_tz":-210,"elapsed":2150,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["import numpy as np\n","import keras\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.optimizers import Adam"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"LN40n1Zxz7Qp","colab_type":"text"},"cell_type":"markdown","source":["<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","در این تمرین میخواهیم از مجموعه داده iris استفاده کنیم.\n","<br>\n","توضیحات این مجموعه داده در سایت آن موجود است:\n","</div>\n","\n","https://archive.ics.uci.edu/ml/datasets/iris\n","\n","\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","ویژگی ها و کلاس های این مجموعه داده به شرح زیر است:\n","</div>\n","\n","Attribute Information:\n","\n","1. sepal length in cm\n","2. sepal width in cm\n","3. petal length in cm\n","4. petal width in cm\n","\n","class:\n","\n","    Iris Setosa\n","    Iris Versicolour\n","    Iris Virginica\n","\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","این دیتاست در کتابخانه sklearn موجود است\n","<br>\n","در قطعه کد زیر ویژگی ها را در x و برچسب یا labelهای متناظر را در y لود شده است.\n","</div>"]},{"metadata":{"id":"DOc682lrz7Qq","colab_type":"code","colab":{}},"cell_type":"code","source":["iris_data = load_iris() # load the iris dataset\n","x = iris_data.data\n","y = iris_data.target.reshape(-1, 1) # Convert data to a single column"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dK-fragez7Qw","colab_type":"text"},"cell_type":"markdown","source":["# <div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\">سوال 1:</div>\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","برچسب یا label های ما در حال حاضر عددی است.\n","<br>\n","این اعداد 0 تا 2 هستند و به عبارتی 3 حالت مختلف دارند.\n","<br>\n","این برچسب ها را به فرمت one-hot تبدیل کنید و خروجی را مجدد در y بریزید.\n","\n","<br>\n","<b>راهنمایی: </b>\n","از تابع keras.utils.to_categorical استفاده کنید.\n","</div>"]},{"metadata":{"id":"ReEGGo_Bz7Qw","colab_type":"code","colab":{}},"cell_type":"code","source":["y = keras.utils.to_categorical(y, num_classes=3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5z6CHAnYz7Q0","colab_type":"text"},"cell_type":"markdown","source":["<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","در زیر داده ها  به داده های test و train تقسیم شده است:\n","</div>"]},{"metadata":{"id":"6uDHL8lGz7Q1","colab_type":"code","colab":{}},"cell_type":"code","source":["# Split the data for training and testing\n","train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o45SmJiSz7Q3","colab_type":"text"},"cell_type":"markdown","source":["# <div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\">سوال 2:</div>\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","یک شبکه با دو hidden-layer در هر لایه 10 نوران و تابع فعالیت relu بسازید. یک لایه Dropout با نرخ 0.5 در لایه آخر ماقبل softmax نیز اضافه کنید.\n","</div>"]},{"metadata":{"id":"8dJ0Hw3Qz7Q4","colab_type":"code","colab":{}},"cell_type":"code","source":["np.random.seed(2)\n","# Build the model\n","\n","model = Sequential()\n","model.add(Dense(10, activation='relu', input_dim=4))\n","model.add(Dense(10, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(3, activation='softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-CwyD2Z-z7Q8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"ca05d3ce-0511-47f7-b6cf-ebfd733e3413","executionInfo":{"status":"ok","timestamp":1549910491911,"user_tz":-210,"elapsed":963,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["model.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_7 (Dense)              (None, 10)                50        \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 10)                110       \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 10)                0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 3)                 33        \n","=================================================================\n","Total params: 193\n","Trainable params: 193\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"TOEp6oZpz7RC","colab_type":"text"},"cell_type":"markdown","source":["<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">در زیر مدل کامپایل شده است.</div>"]},{"metadata":{"id":"SmRZsLqmz7RE","colab_type":"code","colab":{}},"cell_type":"code","source":["# Adam optimizer with learning rate of 0.001\n","optimizer = Adam(lr=0.001)\n","model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uhM9k3iIz7RG","colab_type":"text"},"cell_type":"markdown","source":["# <div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\">سوال 3:</div>\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","مدل را با  batch_size=5 و تعداد 200 ایپاک آموزش دهید.\n","<br>\n","<b>راهنمایی: </b>\n","از تابع model.fit استفاده کنید.\n","</div>"]},{"metadata":{"id":"caiY0TZtz7RI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":6905},"outputId":"2ec45326-e0e3-4acf-eab5-83d7a7e5aeb3","executionInfo":{"status":"ok","timestamp":1549910523135,"user_tz":-210,"elapsed":25624,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["# Train the model\n","model.fit(train_x, train_y,\n","              epochs=200,\n","              batch_size=5)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch 1/200\n","120/120 [==============================] - 2s 20ms/step - loss: 1.2018 - acc: 0.3417\n","Epoch 2/200\n","120/120 [==============================] - 0s 839us/step - loss: 1.0791 - acc: 0.3917\n","Epoch 3/200\n","120/120 [==============================] - 0s 821us/step - loss: 1.0790 - acc: 0.3500\n","Epoch 4/200\n","120/120 [==============================] - 0s 798us/step - loss: 1.0730 - acc: 0.4000\n","Epoch 5/200\n","120/120 [==============================] - 0s 915us/step - loss: 0.9720 - acc: 0.4417\n","Epoch 6/200\n","120/120 [==============================] - 0s 839us/step - loss: 0.9686 - acc: 0.4750\n","Epoch 7/200\n","120/120 [==============================] - 0s 863us/step - loss: 0.9312 - acc: 0.4667\n","Epoch 8/200\n","120/120 [==============================] - 0s 823us/step - loss: 0.9015 - acc: 0.4917\n","Epoch 9/200\n","120/120 [==============================] - 0s 846us/step - loss: 0.8463 - acc: 0.5167\n","Epoch 10/200\n","120/120 [==============================] - 0s 859us/step - loss: 0.8488 - acc: 0.5250\n","Epoch 11/200\n","120/120 [==============================] - 0s 815us/step - loss: 0.7769 - acc: 0.5583\n","Epoch 12/200\n","120/120 [==============================] - 0s 852us/step - loss: 0.7075 - acc: 0.6583\n","Epoch 13/200\n","120/120 [==============================] - 0s 819us/step - loss: 0.7127 - acc: 0.6417\n","Epoch 14/200\n","120/120 [==============================] - 0s 910us/step - loss: 0.7393 - acc: 0.6000\n","Epoch 15/200\n","120/120 [==============================] - 0s 878us/step - loss: 0.7037 - acc: 0.5583\n","Epoch 16/200\n","120/120 [==============================] - 0s 806us/step - loss: 0.6257 - acc: 0.6917\n","Epoch 17/200\n","120/120 [==============================] - 0s 801us/step - loss: 0.7364 - acc: 0.5750\n","Epoch 18/200\n","120/120 [==============================] - 0s 863us/step - loss: 0.6580 - acc: 0.6250\n","Epoch 19/200\n","120/120 [==============================] - 0s 889us/step - loss: 0.6468 - acc: 0.6250\n","Epoch 20/200\n","120/120 [==============================] - 0s 989us/step - loss: 0.6180 - acc: 0.6333\n","Epoch 21/200\n","120/120 [==============================] - 0s 912us/step - loss: 0.6309 - acc: 0.6417\n","Epoch 22/200\n","120/120 [==============================] - 0s 901us/step - loss: 0.7057 - acc: 0.5250\n","Epoch 23/200\n","120/120 [==============================] - 0s 906us/step - loss: 0.6943 - acc: 0.5583\n","Epoch 24/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.6171 - acc: 0.6167\n","Epoch 25/200\n","120/120 [==============================] - 0s 876us/step - loss: 0.5661 - acc: 0.6667\n","Epoch 26/200\n","120/120 [==============================] - 0s 903us/step - loss: 0.6475 - acc: 0.6250\n","Epoch 27/200\n","120/120 [==============================] - 0s 923us/step - loss: 0.5569 - acc: 0.6333\n","Epoch 28/200\n","120/120 [==============================] - 0s 902us/step - loss: 0.6233 - acc: 0.5917\n","Epoch 29/200\n","120/120 [==============================] - 0s 912us/step - loss: 0.6724 - acc: 0.5250\n","Epoch 30/200\n","120/120 [==============================] - 0s 948us/step - loss: 0.5902 - acc: 0.6083\n","Epoch 31/200\n","120/120 [==============================] - 0s 905us/step - loss: 0.6091 - acc: 0.6000\n","Epoch 32/200\n","120/120 [==============================] - 0s 897us/step - loss: 0.6764 - acc: 0.5417\n","Epoch 33/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.7154 - acc: 0.4750\n","Epoch 34/200\n","120/120 [==============================] - 0s 902us/step - loss: 0.6316 - acc: 0.5500\n","Epoch 35/200\n","120/120 [==============================] - 0s 894us/step - loss: 0.5877 - acc: 0.5667\n","Epoch 36/200\n","120/120 [==============================] - 0s 896us/step - loss: 0.6112 - acc: 0.6000\n","Epoch 37/200\n","120/120 [==============================] - 0s 904us/step - loss: 0.6446 - acc: 0.5833\n","Epoch 38/200\n","120/120 [==============================] - 0s 897us/step - loss: 0.5857 - acc: 0.5750\n","Epoch 39/200\n","120/120 [==============================] - 0s 986us/step - loss: 0.6101 - acc: 0.6083\n","Epoch 40/200\n","120/120 [==============================] - 0s 911us/step - loss: 0.5982 - acc: 0.6000\n","Epoch 41/200\n","120/120 [==============================] - 0s 924us/step - loss: 0.5930 - acc: 0.6083\n","Epoch 42/200\n","120/120 [==============================] - 0s 973us/step - loss: 0.6086 - acc: 0.6167\n","Epoch 43/200\n","120/120 [==============================] - 0s 939us/step - loss: 0.5842 - acc: 0.6167\n","Epoch 44/200\n","120/120 [==============================] - 0s 870us/step - loss: 0.6257 - acc: 0.5917\n","Epoch 45/200\n","120/120 [==============================] - 0s 971us/step - loss: 0.5756 - acc: 0.6667\n","Epoch 46/200\n","120/120 [==============================] - 0s 868us/step - loss: 0.5881 - acc: 0.6250\n","Epoch 47/200\n","120/120 [==============================] - 0s 878us/step - loss: 0.5970 - acc: 0.5917\n","Epoch 48/200\n","120/120 [==============================] - 0s 894us/step - loss: 0.5226 - acc: 0.6833\n","Epoch 49/200\n","120/120 [==============================] - 0s 884us/step - loss: 0.5773 - acc: 0.6250\n","Epoch 50/200\n","120/120 [==============================] - 0s 887us/step - loss: 0.6225 - acc: 0.5750\n","Epoch 51/200\n","120/120 [==============================] - 0s 860us/step - loss: 0.5753 - acc: 0.6833\n","Epoch 52/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.6102 - acc: 0.5833\n","Epoch 53/200\n","120/120 [==============================] - 0s 915us/step - loss: 0.6131 - acc: 0.6000\n","Epoch 54/200\n","120/120 [==============================] - 0s 889us/step - loss: 0.6047 - acc: 0.6333\n","Epoch 55/200\n","120/120 [==============================] - 0s 886us/step - loss: 0.6263 - acc: 0.6083\n","Epoch 56/200\n","120/120 [==============================] - 0s 896us/step - loss: 0.5891 - acc: 0.6250\n","Epoch 57/200\n","120/120 [==============================] - 0s 895us/step - loss: 0.5589 - acc: 0.7083\n","Epoch 58/200\n","120/120 [==============================] - 0s 869us/step - loss: 0.5719 - acc: 0.6583\n","Epoch 59/200\n","120/120 [==============================] - 0s 901us/step - loss: 0.5321 - acc: 0.6500\n","Epoch 60/200\n","120/120 [==============================] - 0s 894us/step - loss: 0.5720 - acc: 0.6500\n","Epoch 61/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.5204 - acc: 0.7250\n","Epoch 62/200\n","120/120 [==============================] - 0s 891us/step - loss: 0.6466 - acc: 0.5250\n","Epoch 63/200\n","120/120 [==============================] - 0s 934us/step - loss: 0.5624 - acc: 0.6667\n","Epoch 64/200\n","120/120 [==============================] - 0s 938us/step - loss: 0.6252 - acc: 0.6167\n","Epoch 65/200\n","120/120 [==============================] - 0s 908us/step - loss: 0.6146 - acc: 0.6333\n","Epoch 66/200\n","120/120 [==============================] - 0s 911us/step - loss: 0.6291 - acc: 0.6000\n","Epoch 67/200\n","120/120 [==============================] - 0s 913us/step - loss: 0.5771 - acc: 0.7167\n","Epoch 68/200\n","120/120 [==============================] - 0s 977us/step - loss: 0.5575 - acc: 0.6667\n","Epoch 69/200\n","120/120 [==============================] - 0s 889us/step - loss: 0.5214 - acc: 0.7083\n","Epoch 70/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.5978 - acc: 0.6500\n","Epoch 71/200\n","120/120 [==============================] - 0s 910us/step - loss: 0.5484 - acc: 0.6917\n","Epoch 72/200\n","120/120 [==============================] - 0s 901us/step - loss: 0.6031 - acc: 0.6167\n","Epoch 73/200\n","120/120 [==============================] - 0s 896us/step - loss: 0.5370 - acc: 0.7333\n","Epoch 74/200\n","120/120 [==============================] - 0s 900us/step - loss: 0.5712 - acc: 0.6583\n","Epoch 75/200\n","120/120 [==============================] - 0s 897us/step - loss: 0.5748 - acc: 0.6750\n","Epoch 76/200\n","120/120 [==============================] - 0s 883us/step - loss: 0.5771 - acc: 0.6917\n","Epoch 77/200\n","120/120 [==============================] - 0s 906us/step - loss: 0.5601 - acc: 0.6750\n","Epoch 78/200\n","120/120 [==============================] - 0s 995us/step - loss: 0.5531 - acc: 0.7167\n","Epoch 79/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.5612 - acc: 0.6333\n","Epoch 80/200\n","120/120 [==============================] - 0s 889us/step - loss: 0.5784 - acc: 0.6917\n","Epoch 81/200\n","120/120 [==============================] - 0s 931us/step - loss: 0.5812 - acc: 0.6500\n","Epoch 82/200\n","120/120 [==============================] - 0s 919us/step - loss: 0.5852 - acc: 0.7000\n","Epoch 83/200\n","120/120 [==============================] - 0s 883us/step - loss: 0.5900 - acc: 0.7000\n","Epoch 84/200\n","120/120 [==============================] - 0s 871us/step - loss: 0.5413 - acc: 0.7333\n","Epoch 85/200\n","120/120 [==============================] - 0s 877us/step - loss: 0.5445 - acc: 0.6250\n","Epoch 86/200\n","120/120 [==============================] - 0s 906us/step - loss: 0.5658 - acc: 0.7167\n","Epoch 87/200\n","120/120 [==============================] - 0s 904us/step - loss: 0.5281 - acc: 0.7000\n","Epoch 88/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.4946 - acc: 0.7500\n","Epoch 89/200\n","120/120 [==============================] - 0s 896us/step - loss: 0.5273 - acc: 0.7333\n","Epoch 90/200\n","120/120 [==============================] - 0s 880us/step - loss: 0.5522 - acc: 0.6917\n","Epoch 91/200\n","120/120 [==============================] - 0s 892us/step - loss: 0.5648 - acc: 0.7250\n","Epoch 92/200\n","120/120 [==============================] - 0s 891us/step - loss: 0.5568 - acc: 0.6917\n","Epoch 93/200\n","120/120 [==============================] - 0s 914us/step - loss: 0.5741 - acc: 0.6917\n","Epoch 94/200\n","120/120 [==============================] - 0s 905us/step - loss: 0.5151 - acc: 0.7417\n","Epoch 95/200\n","120/120 [==============================] - 0s 907us/step - loss: 0.5221 - acc: 0.7583\n","Epoch 96/200\n","120/120 [==============================] - 0s 902us/step - loss: 0.5226 - acc: 0.7500\n","Epoch 97/200\n","120/120 [==============================] - 0s 965us/step - loss: 0.5012 - acc: 0.7667\n","Epoch 98/200\n","120/120 [==============================] - 0s 947us/step - loss: 0.5345 - acc: 0.7667\n","Epoch 99/200\n","120/120 [==============================] - 0s 931us/step - loss: 0.5791 - acc: 0.7167\n","Epoch 100/200\n","120/120 [==============================] - 0s 917us/step - loss: 0.4633 - acc: 0.8417\n","Epoch 101/200\n","120/120 [==============================] - 0s 926us/step - loss: 0.5586 - acc: 0.7417\n","Epoch 102/200\n","120/120 [==============================] - 0s 900us/step - loss: 0.5241 - acc: 0.8000\n","Epoch 103/200\n","120/120 [==============================] - 0s 881us/step - loss: 0.5790 - acc: 0.6917\n","Epoch 104/200\n","120/120 [==============================] - 0s 873us/step - loss: 0.4451 - acc: 0.8333\n","Epoch 105/200\n","120/120 [==============================] - 0s 889us/step - loss: 0.5169 - acc: 0.7250\n","Epoch 106/200\n","120/120 [==============================] - 0s 890us/step - loss: 0.5089 - acc: 0.7417\n","Epoch 107/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.5054 - acc: 0.7667\n","Epoch 108/200\n","120/120 [==============================] - 0s 902us/step - loss: 0.5367 - acc: 0.7333\n","Epoch 109/200\n","120/120 [==============================] - 0s 983us/step - loss: 0.5012 - acc: 0.7750\n","Epoch 110/200\n","120/120 [==============================] - 0s 889us/step - loss: 0.4326 - acc: 0.8500\n","Epoch 111/200\n","120/120 [==============================] - 0s 928us/step - loss: 0.5219 - acc: 0.7333\n","Epoch 112/200\n","120/120 [==============================] - 0s 861us/step - loss: 0.5436 - acc: 0.7500\n","Epoch 113/200\n","120/120 [==============================] - 0s 883us/step - loss: 0.4691 - acc: 0.8000\n","Epoch 114/200\n","120/120 [==============================] - 0s 882us/step - loss: 0.4221 - acc: 0.8083\n","Epoch 115/200\n","120/120 [==============================] - 0s 885us/step - loss: 0.4475 - acc: 0.8167\n","Epoch 116/200\n","120/120 [==============================] - 0s 954us/step - loss: 0.5694 - acc: 0.7417\n","Epoch 117/200\n","120/120 [==============================] - 0s 889us/step - loss: 0.4176 - acc: 0.8417\n","Epoch 118/200\n","120/120 [==============================] - 0s 904us/step - loss: 0.4721 - acc: 0.8000\n","Epoch 119/200\n","120/120 [==============================] - 0s 931us/step - loss: 0.4568 - acc: 0.8333\n","Epoch 120/200\n","120/120 [==============================] - 0s 900us/step - loss: 0.4464 - acc: 0.8250\n","Epoch 121/200\n","120/120 [==============================] - 0s 979us/step - loss: 0.5156 - acc: 0.7917\n","Epoch 122/200\n","120/120 [==============================] - 0s 887us/step - loss: 0.4883 - acc: 0.7667\n","Epoch 123/200\n","120/120 [==============================] - 0s 982us/step - loss: 0.4283 - acc: 0.8500\n","Epoch 124/200\n","120/120 [==============================] - 0s 987us/step - loss: 0.4148 - acc: 0.8333\n","Epoch 125/200\n","120/120 [==============================] - 0s 956us/step - loss: 0.4608 - acc: 0.8167\n","Epoch 126/200\n","120/120 [==============================] - 0s 900us/step - loss: 0.4748 - acc: 0.7750\n","Epoch 127/200\n","120/120 [==============================] - 0s 921us/step - loss: 0.4773 - acc: 0.7917\n","Epoch 128/200\n","120/120 [==============================] - 0s 930us/step - loss: 0.4389 - acc: 0.8583\n","Epoch 129/200\n","120/120 [==============================] - 0s 937us/step - loss: 0.5224 - acc: 0.7750\n","Epoch 130/200\n","120/120 [==============================] - 0s 931us/step - loss: 0.4833 - acc: 0.7667\n","Epoch 131/200\n","120/120 [==============================] - 0s 913us/step - loss: 0.4166 - acc: 0.8417\n","Epoch 132/200\n","120/120 [==============================] - 0s 888us/step - loss: 0.5009 - acc: 0.7917\n","Epoch 133/200\n","120/120 [==============================] - 0s 997us/step - loss: 0.5063 - acc: 0.7667\n","Epoch 134/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.4781 - acc: 0.8000\n","Epoch 135/200\n","120/120 [==============================] - 0s 888us/step - loss: 0.5104 - acc: 0.8083\n","Epoch 136/200\n","120/120 [==============================] - 0s 900us/step - loss: 0.3972 - acc: 0.8500\n","Epoch 137/200\n","120/120 [==============================] - 0s 877us/step - loss: 0.4946 - acc: 0.8000\n","Epoch 138/200\n","120/120 [==============================] - 0s 963us/step - loss: 0.4361 - acc: 0.8167\n","Epoch 139/200\n","120/120 [==============================] - 0s 888us/step - loss: 0.4461 - acc: 0.8167\n","Epoch 140/200\n","120/120 [==============================] - 0s 910us/step - loss: 0.4060 - acc: 0.8500\n","Epoch 141/200\n","120/120 [==============================] - 0s 909us/step - loss: 0.4300 - acc: 0.8167\n","Epoch 142/200\n","120/120 [==============================] - 0s 875us/step - loss: 0.4298 - acc: 0.8250\n","Epoch 143/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.4290 - acc: 0.8417\n","Epoch 144/200\n","120/120 [==============================] - 0s 890us/step - loss: 0.3879 - acc: 0.8833\n","Epoch 145/200\n","120/120 [==============================] - 0s 903us/step - loss: 0.4760 - acc: 0.7917\n","Epoch 146/200\n","120/120 [==============================] - 0s 977us/step - loss: 0.4809 - acc: 0.8083\n","Epoch 147/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.4937 - acc: 0.8000\n","Epoch 148/200\n","120/120 [==============================] - 0s 919us/step - loss: 0.4327 - acc: 0.8333\n","Epoch 149/200\n","120/120 [==============================] - 0s 879us/step - loss: 0.4148 - acc: 0.8583\n","Epoch 150/200\n","120/120 [==============================] - 0s 913us/step - loss: 0.4132 - acc: 0.8250\n","Epoch 151/200\n","120/120 [==============================] - 0s 894us/step - loss: 0.4400 - acc: 0.8417\n","Epoch 152/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.3626 - acc: 0.8583\n","Epoch 153/200\n","120/120 [==============================] - 0s 883us/step - loss: 0.4598 - acc: 0.7833\n","Epoch 154/200\n","120/120 [==============================] - 0s 912us/step - loss: 0.3957 - acc: 0.8750\n","Epoch 155/200\n","120/120 [==============================] - 0s 909us/step - loss: 0.3964 - acc: 0.8500\n","Epoch 156/200\n","120/120 [==============================] - 0s 895us/step - loss: 0.4123 - acc: 0.8333\n","Epoch 157/200\n","120/120 [==============================] - 0s 874us/step - loss: 0.4088 - acc: 0.8417\n","Epoch 158/200\n","120/120 [==============================] - 0s 898us/step - loss: 0.3868 - acc: 0.8167\n","Epoch 159/200\n","120/120 [==============================] - 0s 912us/step - loss: 0.3804 - acc: 0.8500\n","Epoch 160/200\n","120/120 [==============================] - 0s 897us/step - loss: 0.4046 - acc: 0.8167\n","Epoch 161/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.4160 - acc: 0.8167\n","Epoch 162/200\n","120/120 [==============================] - 0s 935us/step - loss: 0.4181 - acc: 0.8167\n","Epoch 163/200\n","120/120 [==============================] - 0s 933us/step - loss: 0.3940 - acc: 0.8500\n","Epoch 164/200\n","120/120 [==============================] - 0s 910us/step - loss: 0.3744 - acc: 0.8583\n","Epoch 165/200\n","120/120 [==============================] - 0s 914us/step - loss: 0.3696 - acc: 0.8917\n","Epoch 166/200\n","120/120 [==============================] - 0s 905us/step - loss: 0.4071 - acc: 0.8500\n","Epoch 167/200\n","120/120 [==============================] - 0s 900us/step - loss: 0.5374 - acc: 0.7500\n","Epoch 168/200\n","120/120 [==============================] - 0s 879us/step - loss: 0.5245 - acc: 0.7667\n","Epoch 169/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.4888 - acc: 0.7833\n","Epoch 170/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.4284 - acc: 0.8250\n","Epoch 171/200\n","120/120 [==============================] - 0s 928us/step - loss: 0.4657 - acc: 0.7750\n","Epoch 172/200\n","120/120 [==============================] - 0s 878us/step - loss: 0.3911 - acc: 0.8500\n","Epoch 173/200\n","120/120 [==============================] - 0s 919us/step - loss: 0.4529 - acc: 0.8000\n","Epoch 174/200\n","120/120 [==============================] - 0s 895us/step - loss: 0.4094 - acc: 0.8250\n","Epoch 175/200\n","120/120 [==============================] - 0s 895us/step - loss: 0.4099 - acc: 0.8417\n","Epoch 176/200\n","120/120 [==============================] - 0s 933us/step - loss: 0.4351 - acc: 0.8000\n","Epoch 177/200\n","120/120 [==============================] - 0s 899us/step - loss: 0.3734 - acc: 0.8583\n","Epoch 178/200\n","120/120 [==============================] - 0s 878us/step - loss: 0.4559 - acc: 0.7917\n","Epoch 179/200\n","120/120 [==============================] - 0s 991us/step - loss: 0.4560 - acc: 0.7917\n","Epoch 180/200\n","120/120 [==============================] - 0s 968us/step - loss: 0.4176 - acc: 0.8500\n","Epoch 181/200\n","120/120 [==============================] - 0s 861us/step - loss: 0.3612 - acc: 0.8583\n","Epoch 182/200\n","120/120 [==============================] - 0s 919us/step - loss: 0.4682 - acc: 0.8083\n","Epoch 183/200\n","120/120 [==============================] - 0s 911us/step - loss: 0.3590 - acc: 0.8583\n","Epoch 184/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.4200 - acc: 0.8083\n","Epoch 185/200\n","120/120 [==============================] - 0s 875us/step - loss: 0.4249 - acc: 0.8167\n","Epoch 186/200\n","120/120 [==============================] - 0s 884us/step - loss: 0.4084 - acc: 0.8167\n","Epoch 187/200\n","120/120 [==============================] - 0s 906us/step - loss: 0.4248 - acc: 0.8250\n","Epoch 188/200\n","120/120 [==============================] - 0s 951us/step - loss: 0.3606 - acc: 0.8667\n","Epoch 189/200\n","120/120 [==============================] - 0s 897us/step - loss: 0.4234 - acc: 0.8333\n","Epoch 190/200\n","120/120 [==============================] - 0s 893us/step - loss: 0.3418 - acc: 0.9000\n","Epoch 191/200\n","120/120 [==============================] - 0s 902us/step - loss: 0.3356 - acc: 0.9000\n","Epoch 192/200\n","120/120 [==============================] - 0s 913us/step - loss: 0.3611 - acc: 0.8250\n","Epoch 193/200\n","120/120 [==============================] - 0s 858us/step - loss: 0.3501 - acc: 0.8667\n","Epoch 194/200\n","120/120 [==============================] - 0s 889us/step - loss: 0.3993 - acc: 0.8083\n","Epoch 195/200\n","120/120 [==============================] - 0s 905us/step - loss: 0.4467 - acc: 0.7917\n","Epoch 196/200\n","120/120 [==============================] - 0s 922us/step - loss: 0.4109 - acc: 0.8167\n","Epoch 197/200\n","120/120 [==============================] - 0s 920us/step - loss: 0.3846 - acc: 0.8083\n","Epoch 198/200\n","120/120 [==============================] - 0s 1ms/step - loss: 0.3518 - acc: 0.8417\n","Epoch 199/200\n","120/120 [==============================] - 0s 887us/step - loss: 0.3498 - acc: 0.8833\n","Epoch 200/200\n","120/120 [==============================] - 0s 893us/step - loss: 0.3276 - acc: 0.8833\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fc66cc478d0>"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"dB-TdRviz7RP","colab_type":"text"},"cell_type":"markdown","source":["# <div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\">سوال 4:</div>\n","<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n","مدل را روی داده های test ارزیابی کنید.\n","<br>\n","<b>راهنمایی: </b>\n","از تابع model.evaluate استفاده کنید.\n","</div>"]},{"metadata":{"id":"-ZUITpocz7RR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"77f27ac6-5a15-41d7-8d8c-54259f33240d","executionInfo":{"status":"ok","timestamp":1549910567197,"user_tz":-210,"elapsed":977,"user":{"displayName":"","photoUrl":"","userId":""}}},"cell_type":"code","source":["# Test on unseen data\n","results = model.evaluate(test_x, test_y)\n","\n","print('Final test set loss: {:4f}'.format(results[0]))\n","print('Final test set accuracy: {:4f}'.format(results[1]))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["\r30/30 [==============================] - 0s 2ms/step\n","Final test set loss: 0.255955\n","Final test set accuracy: 0.966667\n"],"name":"stdout"}]},{"metadata":{"id":"bH32Kawaz7RV","colab_type":"text"},"cell_type":"markdown","source":["<div class=\"alert alert-block alert-info\">\n","<div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\"> دوره مقدماتی یادگیری عمیق<br>علیرضا اخوان پور<br>پنج شنبه، ۱۸ و ۲۵ بهمن ۱۳۹۷<br>\n","</div>\n","<a href=\"http://class.vision\">Class.Vision</a> - <a href=\"http://AkhavanPour.ir\">AkhavanPour.ir</a> - <a href=\"https://github.com/Alireza-Akhavan/\">GitHub</a>\n","\n","</div>"]}]}