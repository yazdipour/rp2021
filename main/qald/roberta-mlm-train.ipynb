{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1c3fbe",
   "metadata": {},
   "source": [
    "As the model is BERT-like, weâ€™ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31870f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -r params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed91cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's re-create our tokenizer in transformers\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_dir, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb336a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "config = RobertaConfig(\n",
    "    vocab_size= tokenizer.vocab_size, #52_000,\n",
    "    max_position_embeddings=400, #514,\n",
    "    num_attention_heads=12, #12,\n",
    "    num_hidden_layers=6, #6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f688fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bff6b200e91eadf5\n",
      "Reusing dataset text (/home/shyaz/.cache/huggingface/datasets/text/default-bff6b200e91eadf5/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets -qqq\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "paths = ['../data/04-19-2021-train-sparql.txt', '../data/04-19-2021-test-sparql.txt']\n",
    "# using load_dataset to lazy load data\n",
    "dataset = load_dataset(\"text\", data_files=paths) #text defines the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ce749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lines): return tokenizer(lines['text'], add_special_tokens=True, truncation=True, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132be2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 558\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1967cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_transform(encode)\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e9283aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15) #mlm=MaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375f8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=400,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe2016d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees cuda\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e556d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97fd73da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45359054"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c00291f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d0d3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {model_roberta_mlm} -p\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_roberta_mlm,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c06c65b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/280 00:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.7 s, sys: 5.08 s, total: 59.7 s\n",
      "Wall time: 59.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=280, training_loss=3.1985850742885042, metrics={'train_runtime': 59.5195, 'train_samples_per_second': 4.704, 'total_flos': 19944013171368.0, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f55c6",
   "metadata": {},
   "source": [
    "#### ðŸŽ‰ Save final model (+ tokenizer + config) to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9e09c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_roberta_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa496a",
   "metadata": {},
   "source": [
    "## View Results on Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2de246a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6a000f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs  --host localhost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4934d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard dev upload --logdir runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61ab9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}