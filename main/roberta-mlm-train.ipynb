{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1c3fbe",
   "metadata": {},
   "source": [
    "As the model is BERT-like, weâ€™ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31870f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -r params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb336a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "config = RobertaConfig(\n",
    "    vocab_size=4096, #52_000,\n",
    "    max_position_embeddings=400, #514,\n",
    "    num_attention_heads=6, #12,\n",
    "    num_hidden_layers=3, #6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed91cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's re-create our tokenizer in transformers\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_dir, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f688fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-930acb72d43c1a20\n",
      "Reusing dataset text (/home/shyaz/.cache/huggingface/datasets/text/default-930acb72d43c1a20/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets -qqq\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "paths = ['../data/04-19-2021-train-sparql.txt']\n",
    "# using load_dataset to lazy load data\n",
    "dataset = load_dataset(\"text\", data_files=paths) #text defines the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ce749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lines): return tokenizer(lines['text'], add_special_tokens=True, truncation=True, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f1967cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_transform(encode)\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9283aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15) #mlm=MaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375f8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=400,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2016d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees cuda\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e556d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fd73da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45359054"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00291f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0d3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {model_roberta_mlm} -p\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_roberta_mlm,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c06c65b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41/41 00:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.28 s, sys: 324 ms, total: 7.6 s\n",
      "Wall time: 7.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=41, training_loss=4.726131904415968, metrics={'train_runtime': 7.5641, 'train_samples_per_second': 5.42, 'total_flos': 9454641215760.0, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f55c6",
   "metadata": {},
   "source": [
    "#### ðŸŽ‰ Save final model (+ tokenizer + config) to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9e09c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_roberta_mlm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}