{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 't5-small'\n",
    "model_name='sparql-translator-t5-2021-06-20'\n",
    "model_path='../../data/tokenizers/'+model_name\n",
    "ds_path='../../data/dataset/lc-quad-wikidata-2021-06-20'\n",
    "tokenizer_dir = '../../data/tokenizers/lc-quad-wikidata-2021-06-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/lc-quad-2021-06-20.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "raw_datasets = load_from_disk(ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'en': 'Who is the  {country} for {head of state} of {Mahmoud Abbas}',\n",
       " 'sparql': 'select distinct ?sbj where { ?sbj wdt:P35 wd:Q127998 . ?sbj wdt:P31 wd:Q6256 }'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "raw_datasets['test']['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PLAIN text from csv files\n",
    "MyFile=open(path,'w')\n",
    "for d in raw_datasets['test']['translation']:\n",
    "    MyFile.write(d['sparql'])\n",
    "    MyFile.write('\\n')\n",
    "for d in raw_datasets['train']['translation']:\n",
    "    MyFile.write(d['sparql'])\n",
    "    MyFile.write('\\n')\n",
    "MyFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import AddedToken\n",
    "# tokenizer.add_tokens(['slck_token' , AddedToken(\"select\", normalized=False)]) #'<slct>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=path, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['../../data/tokenizers/lc-quad-wikidata-2021-06-20/vocab.json',\n",
       " '../../data/tokenizers/lc-quad-wikidata-2021-06-20/merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "!mkdir {tokenizer_dir}\n",
    "tokenizer.save_model(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[277, 261, 880, 77, 274, 271, 35, 880, 77, 337, 3437, 30, 84, 1774, 13194, 272, 337, 1967, 30, 75, 13208, 67, 75, 23065, 97]\n['select', 'Ġ?', 'ur', 'i', 'Ġwhere', 'Ġ{', '?', 'ur', 'i', 'Ġd', 'bo', ':', 'p', 'ub', 'lis', 'her', 'Ġd', 'br', ':', 'g', 'mt', '_', 'g', 'ames', '}']\n"
     ]
    }
   ],
   "source": [
    "sample = 'SELECT ?uri WHERE {?uri dbo:publisher dbr:GMT_Games}'.lower()\n",
    "t = tokenizer.encode(sample)\n",
    "print(t.ids)\n",
    "print(t.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' where p w .anssbj? . wdt:583al:1331_344}'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "tokenizer.decode([274, 281, 262, 267, 279, 270, 35, 267, 268, 30, 819, 291, 30, 1358, 67, 753, 97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to tokenize '\\n' as a special token you can simply do:\n",
    "# newline_token = tokenizers.AddedToken(\"\\n\", normalized=False)\n",
    "# my_tokenizer.add_tokens(newline_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load your saved tokenizer from the json is:\n",
    "# from transformers import PreTrainedTokenizerFast\n",
    "# tokenizerx = PreTrainedTokenizerFast(tokenizer_file= '../data/tokenizer-2021-05-30/vocab.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('usr')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}