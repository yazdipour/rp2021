{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: keybert 0.2.0 has requirement numpy>=1.18.5, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers sacrebleu -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path='../../../data/dataset/lc-quad-wikidata-2021-10-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset lc_quad (/home/shyaz/.cache/huggingface/datasets/lc_quad/default/2.0.0/139ee1f12aca006669dcc1f282ec02e126c69e7595453db443ab022643d54086)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"lc_quad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NNQT_question': 'What is {child of} of {husband} of {Ranavalona I} ?',\n",
       " 'paraphrased_question': \"What is the name of Ranavalona I's husband's child?\",\n",
       " 'question': \"Who is the child of Ranavalona I's husband?\",\n",
       " 'sparql_dbpedia18': 'SELECT ?answer WHERE { ?statement1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> <http://wikidata.dbpedia.org/resource/Q169794> . ?statement1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> <http://www.wikidata.org/entity/P26>. ?statement1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?X . ?statement2 <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject> ?X. ?statement2 <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate> <http://www.wikidata.org/entity/P22> . ?statement2 <http://www.w3.org/1999/02/22-rdf-syntax-ns#object> ?answer . }',\n",
       " 'sparql_wikidata': 'SELECT ?answer WHERE { wd:Q169794 wdt:P26 ?X . ?X wdt:P22 ?answer}',\n",
       " 'subgraph': 'left-subgraph',\n",
       " 'template': 'C RCD xD . xD RDE ?E',\n",
       " 'template_index': 8,\n",
       " 'uid': 15554}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_dict = {\"ASK\": \"ask\", \"WHERE\": \"where\", \"SELECT\": \"select\", \"{\": \"[\",\"}\": \"]\", }\n",
    "rep_dict2 = {\"{\": \"\",\"}\": \"\" }\n",
    "def replace_all(text, dict):\n",
    "    for i, j in dict.items(): text = text.replace(i, j)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim + Replace LowerCase + Remove weirdly long Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(ds):\n",
    "    col = 'translation'\n",
    "    df = pd.DataFrame(columns=[col])\n",
    "    for d in tqdm(ds):\n",
    "        try:\n",
    "            qry = replace_all(d['sparql_wikidata'], rep_dict).strip()\n",
    "            if len(d['question'])<250: \n",
    "                q = replace_all(d['question'], rep_dict2).strip()\n",
    "                df = df.append({col: {'en':q, 'sparql': qry}}, ignore_index=True)\n",
    "            if d['paraphrased_question']!=[] and len(d['paraphrased_question'])<250:\n",
    "                q = replace_all(d['paraphrased_question'], rep_dict2).strip()\n",
    "                df = df.append({col: {'en':q, 'sparql': qry}}, ignore_index=True)\n",
    "        except: pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4781/4781 [00:17<00:00, 267.54it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = prepare(raw_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 16779/19293 [00:59<00:12, 203.82it/s]"
     ]
    }
   ],
   "source": [
    "df_train = prepare(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'Who is the  country for head of state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'What country is Mahmoud Abbas the head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'What was the population of Somalia in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'As of 2009, how many people lived in S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'Which female actress is the voice over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>{'en': 'Bernard Devoto received the Pulizer Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9543</th>\n",
       "      <td>{'en': 'What is antonym of of spore print colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9544</th>\n",
       "      <td>{'en': 'What is antonym of of spore print colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>{'en': 'Tell me mixture whose name has the wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9546</th>\n",
       "      <td>{'en': 'Let me know blend whose title has the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9547 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            translation\n",
       "0     {'en': 'Who is the  country for head of state ...\n",
       "1     {'en': 'What country is Mahmoud Abbas the head...\n",
       "2     {'en': 'What was the population of Somalia in ...\n",
       "3     {'en': 'As of 2009, how many people lived in S...\n",
       "4     {'en': 'Which female actress is the voice over...\n",
       "...                                                 ...\n",
       "9542  {'en': 'Bernard Devoto received the Pulizer Pr...\n",
       "9543  {'en': 'What is antonym of of spore print colo...\n",
       "9544  {'en': 'What is antonym of of spore print colo...\n",
       "9545  {'en': 'Tell me mixture whose name has the wor...\n",
       "9546  {'en': 'Let me know blend whose title has the ...\n",
       "\n",
       "[9547 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'What periodical literature does Delta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'What is Delta Air Line's periodical li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'Who is the child of Ranavalona I's hus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'What is the name of Ranavalona I's hus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'Is it true Jeff_Bridges occupation Lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38508</th>\n",
       "      <td>{'en': 'IS THE Bubbling POINT OF THE METHANOL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38509</th>\n",
       "      <td>{'en': 'What type of people live in Fresno, Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38510</th>\n",
       "      <td>{'en': 'What sort of individuals live in Fresn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38511</th>\n",
       "      <td>{'en': 'For which film did Anil Kapoor win a S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38512</th>\n",
       "      <td>{'en': 'For which film did Anil Kapoor win a S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38513 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             translation\n",
       "0      {'en': 'What periodical literature does Delta ...\n",
       "1      {'en': 'What is Delta Air Line's periodical li...\n",
       "2      {'en': 'Who is the child of Ranavalona I's hus...\n",
       "3      {'en': 'What is the name of Ranavalona I's hus...\n",
       "4      {'en': 'Is it true Jeff_Bridges occupation Lan...\n",
       "...                                                  ...\n",
       "38508  {'en': 'IS THE Bubbling POINT OF THE METHANOL ...\n",
       "38509  {'en': 'What type of people live in Fresno, Ca...\n",
       "38510  {'en': 'What sort of individuals live in Fresn...\n",
       "38511  {'en': 'For which film did Anil Kapoor win a S...\n",
       "38512  {'en': 'For which film did Anil Kapoor win a S...\n",
       "\n",
       "[38513 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling all\n",
    "df_all  = df_all.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11602</th>\n",
       "      <td>{'en': 'When are Taxon products determined for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31188</th>\n",
       "      <td>{'en': 'When does the twinned administrative b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8430</th>\n",
       "      <td>{'en': 'Where is the workplace of Heinrich von...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9415</th>\n",
       "      <td>{'en': 'Name a historic region whose highest p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7265</th>\n",
       "      <td>{'en': 'What is the surname of Keiko Matsuzaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35762</th>\n",
       "      <td>{'en': 'What prize does the champion of George...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31838</th>\n",
       "      <td>{'en': 'When is national occasion of comes fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27613</th>\n",
       "      <td>{'en': 'As described by Brockhaus and Efron En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18354</th>\n",
       "      <td>{'en': 'Which streak color is strong arrangeme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11946</th>\n",
       "      <td>{'en': 'What rots to Uranium-238, which featur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9612 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             translation\n",
       "11602  {'en': 'When are Taxon products determined for...\n",
       "31188  {'en': 'When does the twinned administrative b...\n",
       "8430   {'en': 'Where is the workplace of Heinrich von...\n",
       "9415   {'en': 'Name a historic region whose highest p...\n",
       "7265   {'en': 'What is the surname of Keiko Matsuzaka...\n",
       "...                                                  ...\n",
       "35762  {'en': 'What prize does the champion of George...\n",
       "31838  {'en': 'When is national occasion of comes fro...\n",
       "27613  {'en': 'As described by Brockhaus and Efron En...\n",
       "18354  {'en': 'Which streak color is strong arrangeme...\n",
       "11946  {'en': 'What rots to Uranium-238, which featur...\n",
       "\n",
       "[9612 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation', '__index_level_0__'],\n",
       "    num_rows: 9612\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_ds = DatasetDict({'train': ds_train, 'test':ds_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_ds.save_to_disk(ds_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
