{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 't5-small'\n",
    "ds_path='../../data/lc-quad-2021-06-20.txt'\n",
    "ds_path='../../data/dataset/lc-quad-wikidata-2021-06-21'\n",
    "tokenizer_dir = '../../data/tokenizers/lc-quad-wikidata-2021-06-20'\n",
    "path = '../../../data/lc-quad-2021-06-20.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5Model, T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = 'select ?value where { wd:Q41166 p:P551 ?s . ?s ps:P551 wd:Q1012481 . ?s pq:P582 ?value}'\n",
    "qr = 'P582'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [276, 3449, 357, 1], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(model_checkpoint)\n",
    "t = tokenizer(qr)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [276, 3449, 357, 1], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(model_checkpoint, vocab_file ='./spm/vocab.json')\n",
    "t = tokenizer(qr)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.train(files=path, vocab_size=5200, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_vocabulary('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir {tokenizer_dir}\n",
    "# tokenizer.save_model(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'P582</s>'"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "tokenizer.decode(t['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['spm/vocab.json', 'spm/merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers.implementations import SentencePieceBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import AlbertTokenizer\n",
    "# Initialize a tokenizer\n",
    "tokenizer = SentencePieceBPETokenizer(add_prefix_space=True)\n",
    "# Customize training\n",
    "tokenizer.train(files=path, \n",
    "                vocab_size=3200, \n",
    "                min_frequency=1, \n",
    "                show_progress=True,)\n",
    "# Saving model\n",
    "tokenizer.save_model(\"spm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Error while initializing BPE: Merges text file invalid at line 24",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4042cca3363b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentencePieceBPETokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"spm/vocab.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerges\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spm/merges.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tokenizers/implementations/sentencepiece_bpe.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, merges, unk_token, replacement, add_prefix_space, dropout, fuse_unk)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmerges\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             tokenizer = Tokenizer(\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mBPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_unk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuse_unk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             )\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error while initializing BPE: Merges text file invalid at line 24"
     ]
    }
   ],
   "source": [
    "tokenizer = SentencePieceBPETokenizer(vocab= \"spm/vocab.json\", merges=\"spm/merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=3200, model=SentencePieceBPE, unk_token=<unk>, replacement=‚ñÅ, add_prefix_space=True, dropout=None)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('usr')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}